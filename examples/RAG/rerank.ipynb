{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU datasets==2.14.5 openai==1.14.3 pinecone-client==3.2.2 cohere==5.2.2 boto3==1.34.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 41584\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"jamescalam/ai-arxiv-chunked\", split=\"train\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '1910.01108',\n",
       " 'chunk-id': '0',\n",
       " 'chunk': 'DistilBERT, a distilled version of BERT: smaller,\\nfaster, cheaper and lighter\\nVictor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\\nHugging Face\\n{victor,lysandre,julien,thomas}@huggingface.co\\nAbstract\\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent\\nin Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains\\nchallenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be ﬁnetuned with good performances on a wide range of tasks like its larger counterparts.\\nWhile most prior work investigated the use of distillation for building task-speciﬁc\\nmodels, we leverage knowledge distillation during the pre-training phase and show\\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\\nof its language understanding capabilities and being 60% faster. To leverage the\\ninductive biases learned by larger models during pre-training, we introduce a triple\\nloss combining language modeling, distillation and cosine-distance losses. Our\\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its',\n",
       " 'id': '1910.01108',\n",
       " 'title': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter',\n",
       " 'summary': 'As Transfer Learning from large-scale pre-trained models becomes more\\nprevalent in Natural Language Processing (NLP), operating these large models in\\non-the-edge and/or under constrained computational training or inference\\nbudgets remains challenging. In this work, we propose a method to pre-train a\\nsmaller general-purpose language representation model, called DistilBERT, which\\ncan then be fine-tuned with good performances on a wide range of tasks like its\\nlarger counterparts. While most prior work investigated the use of distillation\\nfor building task-specific models, we leverage knowledge distillation during\\nthe pre-training phase and show that it is possible to reduce the size of a\\nBERT model by 40%, while retaining 97% of its language understanding\\ncapabilities and being 60% faster. To leverage the inductive biases learned by\\nlarger models during pre-training, we introduce a triple loss combining\\nlanguage modeling, distillation and cosine-distance losses. Our smaller, faster\\nand lighter model is cheaper to pre-train and we demonstrate its capabilities\\nfor on-device computations in a proof-of-concept experiment and a comparative\\non-device study.',\n",
       " 'source': 'http://arxiv.org/pdf/1910.01108',\n",
       " 'authors': ['Victor Sanh',\n",
       "  'Lysandre Debut',\n",
       "  'Julien Chaumond',\n",
       "  'Thomas Wolf'],\n",
       " 'categories': ['cs.CL'],\n",
       " 'comment': 'February 2020 - Revision: fix bug in evaluation metrics, updated\\n  metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at\\n  the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing\\n  - NeurIPS 2019',\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.CL',\n",
       " 'published': '20191002',\n",
       " 'updated': '20200301',\n",
       " 'references': [{'id': '1910.01108'}]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'metadata'],\n",
       "    num_rows: 41584\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.map(lambda x: {\n",
    "    \"id\": f'{x[\"id\"]}-{x[\"chunk-id\"]}',\n",
    "    \"text\": x[\"chunk\"],\n",
    "    \"metadata\": {\n",
    "        \"title\": x[\"title\"],\n",
    "        \"url\": x[\"source\"],\n",
    "        \"primary_category\": x[\"primary_category\"],\n",
    "        \"published\": x[\"published\"],\n",
    "        \"updated\": x[\"updated\"],\n",
    "        \"text\": x[\"chunk\"],\n",
    "    }\n",
    "})\n",
    "# drop uneeded columns\n",
    "data = data.remove_columns([\n",
    "    \"title\", \"summary\", \"source\",\n",
    "    \"authors\", \"categories\", \"comment\",\n",
    "    \"journal_ref\", \"primary_category\",\n",
    "    \"published\", \"updated\", \"references\",\n",
    "    \"doi\", \"chunk-id\",\n",
    "    \"chunk\"\n",
    "])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py310/lib/python3.10/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# from pinecone import Pinecone, ServerlessSpec\n",
    "from pinecone import Pinecone, PodSpec\n",
    "pc = Pinecone(\n",
    "    api_key=os.environ.get(\"PINECONE_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"rerank\"\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pc.list_indexes():\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        # in alignment with the number of dimensions in cohere 1024\n",
    "        dimension=1024,\n",
    "        metric=\"cosine\",\n",
    "        spec=PodSpec(\n",
    "            environment=\"gcp-starter\"\n",
    "        )\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to index\n",
    "index_name = \"rerank\"\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "\n",
    "# # use OpenAI ada to get embeddings\n",
    "# def generate_text_embeddings(text, model = \"text-embedding-ada-002\"):\n",
    "#    text = text.replace(\"\\n\", \" \")\n",
    "#    return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "# generate_text_embeddings(\"Hello, world!\")\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import boto3\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def generate_text_embeddings(model_id, body):\n",
    "    \"\"\"\n",
    "    Generate text embedding by using the Cohere Embed model.\n",
    "    Args:\n",
    "        model_id (str): The model ID to use.\n",
    "        body (str) : The reqest body to use.\n",
    "    Returns:\n",
    "        dict: The response from the model.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\n",
    "        \"Generating text emdeddings with the Cohere Embed model %s\", model_id)\n",
    "\n",
    "    accept = '*/*'\n",
    "    content_type = 'application/json'\n",
    "\n",
    "    bedrock = boto3.client(service_name='bedrock-runtime', region_name=\"us-east-1\")\n",
    "\n",
    "    response = bedrock.invoke_model(\n",
    "        body=body,\n",
    "        modelId=model_id,\n",
    "        accept=accept,\n",
    "        contentType=content_type\n",
    "    )\n",
    "\n",
    "    logger.info(\"Successfully generated text with Cohere model %s\", model_id)\n",
    "\n",
    "    return response\n",
    "\n",
    "model_id = 'cohere.embed-multilingual-v3'\n",
    "text1 = \"hello world\"\n",
    "text2 = \"this is a test\"\n",
    "input_type = \"search_document\"\n",
    "\n",
    "def preflight():\n",
    "    try:\n",
    "        body = json.dumps({\n",
    "            \"texts\": [text1, text2],\n",
    "            \"input_type\": input_type}\n",
    "        )\n",
    "        response = generate_text_embeddings(model_id=model_id, body=body)\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "\n",
    "        print(f\"ID: {response_body.get('id')}\")\n",
    "        print(f\"Response type: {response_body.get('response_type')}\")\n",
    "\n",
    "        print(\"Embeddings\")\n",
    "        for i, embedding in enumerate(response_body.get('embeddings')):\n",
    "            print(f\"\\tEmbedding {i}\")\n",
    "            print(*embedding)\n",
    "\n",
    "        print(\"Texts\")\n",
    "        for i, text in enumerate(response_body.get('texts')):\n",
    "            print(f\"\\tText {i}: {text}\")\n",
    "\n",
    "    except ClientError as err:\n",
    "        message = err.response[\"Error\"][\"Message\"]\n",
    "        logger.error(\"A client error occurred: %s\", message)\n",
    "        print(\"A client error occured: \" +\n",
    "                format(message))\n",
    "    else:\n",
    "        print(\n",
    "            f\"Finished generating text embeddings with Cohere model {model_id}.\")\n",
    "\n",
    "# preflight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DistilBERT, a distilled version of BERT: smaller,\\nfaster, cheaper and lighter\\nVictor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\\nHugging Face\\n{victor,lysandre,julien,thomas}@huggingface.co\\nAbstract\\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent\\nin Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains\\nchallenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be ﬁnetuned with good performances on a wide range of tasks like its larger counterparts.\\nWhile most prior work investigated the use of distillation for building task-speciﬁc\\nmodels, we leverage knowledge distillation during the pre-training phase and show\\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\\nof its language understanding capabilities and being 60% faster. To leverage the\\ninductive biases learned by larger models during pre-training, we introduce a triple\\nloss combining language modeling, distillation and cosine-distance losses. Our\\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its',\n",
       " 'loss combining language modeling, distillation and cosine-distance losses. Our\\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its\\ncapabilities for on-device computations in a proof-of-concept experiment and a\\ncomparative on-device study.\\n1 Introduction\\nFigure 1: Parameter counts of several recently released\\npretrained language models.The last two years have seen the rise\\nof Transfer Learning approaches in\\nNatural Language Processing (NLP)\\nwith large-scale pre-trained language\\nmodels becoming a basic tool in\\nmany NLP tasks [Devlin et al., 2018,\\nRadford et al., 2019, Liu et al., 2019].\\nWhile these models lead to signiﬁcant improvement, they often have\\nseveral hundred million parameters\\nand current research1on pre-trained\\nmodels indicates that training even\\nlarger models still leads to better performances on downstream tasks.\\nThe trend toward bigger models\\nraises several concerns. First is the\\nenvironmental cost of exponentially scaling these models’ computational requirements as mentioned\\nin Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device\\nin real-time has the potential to enable novel and interesting language processing applications, the\\ngrowing computational and memory requirements of these models may hamper wide adoption.',\n",
       " 'in real-time has the potential to enable novel and interesting language processing applications, the\\ngrowing computational and memory requirements of these models may hamper wide adoption.\\n1See for instance the recently released MegatronLM ( https://nv-adlr.github.io/MegatronLM )\\nEMC^2: 5th Edition Co-located with NeurIPS’19arXiv:1910.01108v4  [cs.CL]  1 Mar 2020\\nIn this paper, we show that it is possible to reach similar performances on many downstream-tasks\\nusing much smaller language models pre-trained with knowledge distillation, resulting in models\\nthat are lighter and faster at inference time, while also requiring a smaller computational training\\nbudget. Our general-purpose pre-trained models can be ﬁne-tuned with good performances on several\\ndownstream tasks, keeping the ﬂexibility of larger models. We also show that our compressed models\\nare small enough to run on the edge, e.g. on mobile devices.\\nUsing a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained\\nthrough distillation via the supervision of a bigger Transformer language model can achieve similar\\nperformance on a variety of downstream tasks, while being 60% faster at inference time. Further',\n",
       " 'through distillation via the supervision of a bigger Transformer language model can achieve similar\\nperformance on a variety of downstream tasks, while being 60% faster at inference time. Further\\nablation studies indicate that all the components of the triple loss are important for best performances.\\nWe have made the trained weights available along with the training code in the Transformers2\\nlibrary from HuggingFace [Wolf et al., 2019].\\n2 Knowledge distillation\\nKnowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique in which\\na compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher or an ensemble of models.\\nIn supervised learning, a classiﬁcation model is generally trained to predict an instance class by\\nmaximizing the estimated probability of gold labels. A standard training objective thus involves\\nminimizing the cross-entropy between the model’s predicted distribution and the one-hot empirical\\ndistribution of training labels. A model performing well on the training set will predict an output\\ndistribution with high probability on the correct class and with near-zero probabilities on other\\nclasses. But some of these \"near-zero\" probabilities are larger than others and reﬂect, in part, the\\ngeneralization capabilities of the model and how well it will perform on the test set3.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:4][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41584"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 5  # how many embeddings we create and insert at once\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    passed = False\n",
    "    # find end of batch\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    # create batch\n",
    "    batch = data[i:i_end]\n",
    "    # create embeddings (exponential backoff to avoid RateLimitError)\n",
    "    for j in range(5):  # max 5 retries\n",
    "        try:\n",
    "            # res = client.embeddings.create(input=batch[\"text\"], model=embed_model)\n",
    "            body = json.dumps({\n",
    "                \"texts\": batch[\"text\"],\n",
    "                \"input_type\": input_type}\n",
    "            )\n",
    "            res = generate_text_embeddings(model_id=model_id, body=body)\n",
    "            print(f\"Created embeddings for batch {i}-{i_end}.\")\n",
    "            passed = True\n",
    "        except Exception as e:\n",
    "            time.sleep(2**j)  # wait 2^j seconds before retrying\n",
    "            print(\"Retrying...\")\n",
    "    if not passed:\n",
    "        raise RuntimeError(\"Failed to create embeddings.\")\n",
    "    # get embeddings\n",
    "    response_body = json.loads(res.get('body').read())\n",
    "    # for i, embedding in enumerate(response_body.get('embeddings')):\n",
    "    #     print(f\"\\tEmbedding {i}\")\n",
    "    #     print(*embedding)\n",
    "    # embeds = [embedding for i, embedding in enumerate(response_body.get('embeddings'))]\n",
    "    embeds = [float(embedding) for embedding in response_body.get('embeddings')]\n",
    "    to_upsert = list(zip(batch[\"id\"], embeds, batch[\"metadata\"]))\n",
    "    print(f\"Upserting batch {i}-{i_end} to Pinecone with embeds {embeds}\")\n",
    "    # upsert to Pinecone\n",
    "    index.upsert(vectors=to_upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(query: str, top_k: int):\n",
    "    # assemble the body\n",
    "    body = json.dumps({\n",
    "        \"texts\": [query],\n",
    "        \"input_type\": input_type}\n",
    "    )\n",
    "    res = generate_text_embeddings(model_id=model_id, body=body)\n",
    "    response_body = json.loads(res.get('body').read())\n",
    "    # xq = [float(embedding) for embedding in response_body.get('embeddings')]\n",
    "    xq = [[float(value) for value in embedding] for embedding in response_body.get('embeddings')]\n",
    "    print(f\"Created embeddings for query {query} with model {model_id} and the embeddings are {xq}\")\n",
    "    # search pinecone index\n",
    "    res = index.query(vector=xq, top_k=top_k, include_metadata=True)\n",
    "    \"\"\"\n",
    "\n",
    "    {'matches': \n",
    "        [\n",
    "                {'id': '1703.04933-25',\n",
    "                        'metadata': {'primary_category': 'cs.LG',\n",
    "                                'published': '20170315',\n",
    "                                'text': '...',\n",
    "                                'title': 'Sharp Minima Can Generalize For Deep Nets',\n",
    "                                'updated': '20170515',\n",
    "                                'url': 'http://arxiv.org/pdf/1703.04933'},\n",
    "                        'score': 0.588596284,\n",
    "                        'values': []\n",
    "              },\n",
    "              ...\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    # get doc text\n",
    "    docs = {x[\"metadata\"]['text']: i for i, x in enumerate(res[\"matches\"])}\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating text emdeddings with the Cohere Embed model cohere.embed-multilingual-v3\n",
      "INFO:__main__:Successfully generated text with Cohere model cohere.embed-multilingual-v3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings for query can you explain why we would want to do rlhf? with model cohere.embed-multilingual-v3 and the embeddings are [[0.011528015, 0.038635254, 0.0345459, 0.027618408, -0.017654419, 0.012954712, -0.0032405853, -0.03363037, 0.0023975372, -0.04006958, 0.010719299, 0.014305115, -0.008773804, 0.021835327, -0.04385376, 0.017303467, 0.040130615, -0.0043792725, 0.04888916, -0.004753113, -0.027053833, 0.013305664, 0.06011963, -0.05419922, -0.051879883, 0.04498291, -0.0044784546, -0.014053345, 0.022079468, -0.022994995, 0.007904053, -0.0103302, 0.06237793, 0.012413025, 0.027755737, 0.031951904, -0.010765076, -0.028030396, -0.004337311, 0.05456543, 0.00026392937, 0.0031089783, -0.00025773048, 0.020004272, -0.00039863586, 0.004638672, 0.014450073, -0.004722595, -0.060424805, 0.05758667, -0.03942871, 0.015403748, 0.033813477, -0.004016876, 0.015342712, 0.01878357, 0.021118164, -0.0056991577, 0.01612854, -0.0017633438, 0.014228821, -0.0070648193, -0.005718231, 0.01651001, 0.05441284, 0.03805542, 0.0034770966, 0.05053711, -0.011940002, -0.0015268326, 0.04196167, 0.024307251, 0.029632568, 0.0040626526, 0.004295349, 0.0007815361, 0.0070381165, -0.057403564, 0.044799805, 0.03845215, -0.023254395, 0.0068893433, 0.049957275, 0.027191162, -0.03857422, -0.0045394897, 0.01083374, 0.0009121895, -0.0055732727, 0.017364502, 0.007171631, 0.02798462, 0.049804688, 0.015335083, -0.0042800903, 0.0004620552, 0.032287598, -0.0121536255, 0.0047569275, -0.004486084, 0.026412964, -0.0017662048, 0.0059890747, 0.00724411, 0.014205933, 0.028427124, 0.013908386, -0.012207031, -0.096191406, -0.04071045, -0.041412354, -0.007041931, -0.012252808, -0.004047394, -0.021942139, -0.06854248, 0.0010662079, -0.012016296, -0.017578125, -0.03704834, -0.042663574, -0.027709961, 0.0013809204, 0.021057129, -0.006793976, -0.027557373, 0.015411377, -0.03805542, 0.03375244, 0.03555298, -0.029174805, 0.036468506, -0.042266846, 0.0032978058, -0.0041503906, 0.07043457, 0.014167786, -0.020996094, 0.04711914, 0.010406494, -0.0042266846, -0.003742218, -0.006072998, -0.014556885, 0.014190674, 0.0135650635, -0.04763794, 0.036224365, 0.010612488, -0.056640625, 0.01600647, 0.04534912, 0.074157715, -0.019729614, -0.005756378, 0.03668213, 0.021255493, 0.005809784, -0.034942627, 0.062072754, 0.007507324, -0.034484863, -0.045776367, -0.009490967, 0.024871826, -0.01638794, -0.024291992, -0.032440186, 0.018630981, 0.016403198, 0.064453125, -0.04257202, 0.01739502, 0.016189575, -0.009361267, -0.026748657, -0.0079422, 0.0110321045, 0.006099701, -0.052246094, -0.012535095, -0.0038490295, -0.018600464, 0.025848389, 0.009712219, 0.013298035, 0.0042266846, -0.0038757324, 0.028411865, 0.05279541, -0.049926758, 0.00434494, -0.0970459, 0.04763794, -0.029937744, 0.029663086, 0.03930664, -0.05255127, 0.06359863, 0.10687256, -0.015960693, -0.013793945, 0.015419006, 0.02507019, -0.027832031, 0.0044021606, 0.0033130646, 0.034851074, -0.036865234, 0.012641907, -0.009239197, -0.015731812, -0.012046814, -0.0033855438, 0.011520386, 0.037231445, 0.0028324127, 0.037261963, 0.017364502, 0.03024292, 0.04989624, -0.005104065, -0.0013465881, 0.00079631805, -0.004512787, -0.022109985, -0.0050354004, 0.009094238, -0.010536194, -0.0058517456, -0.0053520203, -0.019882202, 0.013389587, 0.003944397, 0.03277588, 0.009742737, -0.0050811768, -0.00919342, -0.028793335, 0.010940552, 0.0012798309, 0.0001693964, 0.035949707, 0.013496399, 0.01423645, 0.031097412, 0.011909485, 0.02784729, -0.018859863, -0.033172607, 0.0073051453, 0.01725769, -0.019638062, -0.023605347, 0.024429321, 0.006095886, -0.07702637, 0.029571533, 0.046722412, 0.010116577, 0.013008118, -0.005241394, 0.04360962, -0.012298584, -0.03213501, -0.008384705, -0.048858643, 0.012016296, 0.06854248, -0.029510498, 0.00919342, -0.032928467, -0.007030487, 0.05203247, 0.028121948, 0.051605225, -0.033355713, 0.049743652, -0.049713135, 0.044036865, 0.010475159, 0.028182983, -0.006500244, -0.008613586, -0.022094727, 0.08215332, -0.0513916, 0.024597168, -0.037628174, 0.005405426, -0.014411926, -0.033355713, -0.014122009, 0.0037975311, -0.019592285, 0.0026454926, 0.03253174, -0.02935791, -0.038635254, -0.00056934357, -0.011802673, -0.026062012, -0.0135269165, -0.029693604, -0.0013465881, 0.018692017, 0.009124756, 0.033355713, 0.004184723, -0.017349243, 0.0006752014, 0.014678955, -0.023269653, 0.01864624, -0.07055664, 0.023208618, -0.053527832, -0.06439209, -0.013328552, 0.033691406, -0.009567261, -0.009361267, -0.038604736, 0.03213501, -0.01802063, 0.0024585724, -0.010925293, -0.0075645447, 0.018707275, -0.0044059753, -0.07775879, -0.014526367, 0.022247314, -0.009513855, -0.009284973, -0.030166626, 0.0033473969, -0.005458832, 0.0007739067, -0.01436615, 0.00010383129, 0.013244629, -0.0099487305, 0.0039787292, 0.041107178, -0.014587402, 0.00844574, -0.08270264, -0.083984375, -0.052459717, 0.021697998, 0.06188965, -0.09857178, -0.028808594, -0.0035743713, 0.021896362, -0.064697266, 0.003932953, -0.013725281, -0.024719238, 0.0259552, 0.009170532, -0.078063965, 0.020492554, 0.00548172, -0.012145996, -0.0041656494, 0.010597229, -0.026382446, 0.044921875, 0.0068473816, 0.032165527, 0.035247803, 0.006011963, -0.027236938, 0.047851562, 0.004924774, 0.019180298, 0.040924072, -0.0045433044, -0.010826111, -0.01763916, 0.03692627, 0.015670776, 0.047180176, 0.03567505, -0.014045715, -0.026855469, 0.0029850006, 0.005317688, 0.07098389, 0.036590576, 0.01499176, 0.025512695, 0.013542175, -0.0046081543, -0.024139404, 0.0076675415, -0.02067566, -0.020507812, 0.041534424, -0.03451538, -0.0096206665, -0.009178162, -0.0027751923, 0.021148682, 0.0021018982, -0.0037384033, 0.02796936, 0.014533997, 0.007980347, -0.003829956, -0.040130615, -0.034698486, -0.011634827, 0.002122879, -0.021011353, 0.026428223, -0.02130127, -0.0121154785, -0.018203735, -0.01637268, -0.0073051453, -0.018432617, -0.032165527, -0.042175293, 0.020370483, -0.043060303, 0.034454346, 0.026443481, 0.05505371, 0.0236969, 0.011795044, -0.013847351, -0.018829346, -0.0006465912, -0.059295654, 0.020141602, -0.016189575, -0.0047416687, 0.007358551, -0.014793396, 0.028076172, -0.015449524, 0.06402588, 0.023757935, -0.018966675, 0.02708435, -0.021514893, 0.029067993, -0.026733398, -0.025650024, -0.014221191, 0.029388428, 0.00434494, 0.02381897, -0.009338379, 0.02015686, 0.034088135, -0.020050049, -0.025817871, -0.010925293, -0.030960083, -0.036499023, 0.035461426, -0.04647827, 0.014709473, 0.022323608, 0.0010700226, 0.020187378, -0.055999756, 0.064208984, -0.019561768, -0.03866577, 0.053100586, 0.047027588, -0.024612427, 0.0046806335, 0.021896362, 0.034179688, -0.028457642, 0.009414673, 0.037475586, -0.016799927, -0.0126953125, 0.021469116, 0.045776367, 0.0027103424, 0.06524658, -0.004180908, 0.010360718, -0.00018084049, -0.06298828, -0.018936157, 0.030517578, -0.03213501, 0.02142334, -0.009513855, 0.03439331, -0.00217247, -0.0154418945, 0.0062332153, 0.043395996, -0.0037460327, -0.0063171387, -0.026367188, 0.015777588, 0.05050659, 0.020324707, 0.026428223, 0.0021018982, -0.034729004, -0.0637207, 0.027053833, -0.0076179504, -0.05316162, 0.032592773, 0.040374756, -0.0023345947, -0.018310547, -0.012557983, 0.040374756, -0.034698486, 0.013633728, 0.046417236, 0.002609253, -0.0680542, -0.042114258, -0.052337646, 0.023986816, -0.021347046, -0.027664185, 0.00033569336, -0.015594482, 0.007232666, -0.037139893, 0.03375244, -0.01675415, 0.050933838, 0.0155181885, 0.025939941, -0.015571594, 0.0039787292, 0.0069236755, 0.014137268, 0.00045251846, -0.049743652, -0.00642395, -0.00459671, 0.043670654, -0.03543091, 0.04537964, -0.022735596, -0.023254395, -0.0049438477, -0.006038666, -0.011054993, -0.0039787292, 0.013221741, 0.018661499, 0.015022278, -0.09313965, -0.00472641, 0.04711914, -0.008590698, 0.014579773, -0.00856781, -0.0072746277, -0.04034424, -0.059906006, 0.020828247, 0.08380127, 0.027557373, 0.019943237, -0.011154175, -0.024719238, 0.026321411, 0.03765869, 0.049072266, -0.025482178, 0.035614014, -0.003730774, -0.011550903, -0.008659363, -0.004722595, 0.028030396, -0.026397705, -0.040039062, -0.004184723, -0.005355835, 0.004650116, -0.046051025, 0.004131317, 0.048339844, -0.01763916, 0.016113281, 0.022384644, 0.01739502, -0.0044136047, 0.0079574585, -0.08557129, -0.018295288, -0.031143188, -0.01486969, 0.010719299, 0.03286743, -0.0051498413, 0.00308609, -0.0074157715, 0.0032367706, 0.024459839, -0.011962891, -0.01939392, 0.022415161, -0.035369873, -0.010726929, 0.0007452965, 0.04168701, 0.008346558, -0.00027608871, -0.016189575, -0.030670166, -0.007381439, -0.0013494492, -0.03515625, -0.011558533, 0.056671143, -0.0022220612, -0.0892334, -0.062164307, -0.03527832, -0.044677734, -0.004890442, -0.025283813, -0.0056610107, -0.0011014938, 0.008605957, -0.043182373, -0.032714844, 0.03161621, -0.01033783, -0.05657959, 0.070617676, -0.041900635, -0.0079956055, -0.04434204, 4.6372414e-05, -0.02708435, -0.028930664, 0.022216797, 0.028427124, 0.04055786, 0.024871826, -0.038848877, -0.0064048767, -0.0010824203, 0.02809143, 0.07244873, 0.004081726, 0.04260254, -0.032409668, -0.012161255, -0.0146484375, -0.025177002, -0.00055646896, -0.040130615, -0.021347046, -0.010757446, 0.008682251, -0.027160645, 0.037750244, 0.0026493073, 0.0004272461, 0.026687622, 0.0390625, 0.0029563904, -0.05456543, -0.047302246, -0.0047302246, -0.035583496, 0.016052246, 0.012130737, -0.021377563, 0.0063552856, -0.004650116, 0.021392822, -0.03375244, -0.0034770966, -0.029464722, 0.008552551, -0.017211914, 0.010879517, -0.016281128, 0.006122589, 0.04788208, -0.0440979, 0.02609253, 0.035583496, -0.04788208, -0.018966675, 0.0013608932, 0.0027599335, -0.03186035, 0.014091492, -0.0018262863, 0.0158844, 0.027145386, -0.014083862, 0.012359619, 0.0075645447, -0.04168701, 0.007408142, -0.052093506, -0.01335144, 0.0027046204, -0.010910034, -0.007080078, 0.034240723, -0.05432129, 0.0043525696, 0.023269653, 0.0038528442, -0.021392822, -0.04824829, 0.0079956055, 0.02835083, -0.039855957, -0.029434204, 0.012397766, -0.005897522, -0.0284729, -0.017807007, -0.0035209656, -0.0016899109, 0.004699707, -0.053894043, -0.040374756, 0.0074310303, 0.06994629, 0.0033550262, 0.05090332, 0.018096924, 0.0013275146, -0.07055664, -0.032684326, 0.04196167, -0.021850586, 0.048614502, -0.026153564, 0.011558533, -0.025680542, 0.015777588, -0.015670776, -0.038909912, -0.000895977, -0.015426636, 0.043395996, -0.042633057, 0.0020999908, 0.068847656, 0.00605011, 0.0075302124, -0.013313293, -0.026733398, -0.021194458, 0.012123108, 0.011253357, -0.023361206, 0.03933716, 0.024841309, 0.00724411, -0.017974854, -0.0154418945, -0.0009293556, 0.015533447, 0.00056791306, 0.03164673, -0.018508911, 0.028869629, -0.037261963, 0.008621216, 0.011482239, 0.0137786865, 0.0059318542, -0.03314209, -0.026382446, 0.024429321, -0.013900757, -0.0042533875, 0.080444336, 0.05682373, -0.00013160706, 0.05355835, 0.008163452, 0.05569458, -0.034210205, 0.0070533752, -0.016830444, 0.022766113, -0.0107803345, 0.06732178, -0.025039673, -0.06298828, 0.010002136, 0.043518066, -0.02279663, -0.058746338, 0.025650024, -0.038146973, 0.028839111, -0.06939697, 0.04837036, -0.027877808, 0.003643036, 0.030960083, 0.018661499, 0.06121826, 0.04208374, -0.003211975, 0.039093018, 0.005870819, 0.0006918907, -0.017242432, 0.011329651, 0.06286621, 0.011703491, 0.013153076, -0.0035076141, 0.020050049, 0.0552063, 0.037750244, 0.010856628, -0.027832031, 0.009811401, -0.025650024, -0.0075531006, 0.0073890686, 0.019561768, 0.01663208, -0.0069732666, 0.007610321, -0.040161133, -0.07434082, 0.017425537, 0.02041626, 0.06573486, 0.012931824, 0.0038394928, 0.015090942, 0.08111572, -0.02368164, 0.05102539, 0.04949951, -0.024276733, -0.012535095, -0.02684021, 7.1406364e-05, -0.015296936, 0.0044670105, -0.016723633, -0.05014038, 0.0048599243, -0.041534424, 0.009025574, 0.015945435, 0.010231018, -0.008628845, 0.030792236, 0.02494812, -0.042297363, -0.023269653, -0.0002617836, 0.082458496, -0.060913086, -0.0043792725, -0.011116028, 0.013130188, -0.037994385, -0.015945435, 0.005760193, 0.035491943, -0.014190674, 0.010734558, -0.048583984, -0.009567261, -0.01826477, -0.04940796, 0.019363403, -0.044067383, 0.0769043, 0.024368286, -0.03729248, 0.03479004, 0.017196655, 0.0093688965, -0.05117798, 0.016403198, -0.085754395, 0.019836426, 0.00044441223, -0.018066406, 0.07922363, -0.048553467, -0.088378906, 0.0010223389, -0.010795593, 0.008728027, 0.0137786865, 0.03111267, -0.026443481, 0.017868042, 0.04736328, -0.039611816, 0.01663208, 0.049835205, -0.0064086914, 0.023101807, -0.0107803345, -0.0067710876, 0.033935547, 0.017288208, 0.04043579, -0.048858643, -0.008987427, -0.017730713, 0.022033691, 0.0023422241, 0.021652222, -0.033966064, 0.03982544, -0.013908386, -0.028137207, 0.012680054, -0.03515625, -0.033325195, 0.007472992, 0.009857178, 0.029678345, 0.07006836, -0.015296936, 0.014968872, 0.060272217, 0.01979065, -0.037017822, 0.009124756, 0.010498047, -0.0051612854, -0.037475586, 0.019500732, 0.009773254, -0.0052986145, 0.048309326, 0.011497498, 0.026107788, 0.011779785, 0.0084991455, -0.012611389, -0.009880066, 0.012924194, -0.020065308, 0.0012235641, 0.05581665, 0.04296875, 0.028793335, 0.0063209534, 0.007507324, -0.036010742, -0.028015137, 0.015457153, 0.03656006, 0.058044434, -0.013557434, -0.033111572, -0.018630981, -0.0003209114, -0.048919678, -0.014633179, -0.0597229, 0.0970459, 0.044647217, -0.007472992, 0.018295288, -0.015434265, 0.0024280548, -0.011604309, 0.049072266, -0.02078247, 0.027389526, -0.0010652542, -0.064941406, 0.008575439, 0.0067863464, -0.022064209, -0.019058228, 0.022415161, 0.026184082, -0.013885498, -0.0019083023, 0.046966553, 0.021591187, 0.009117126, -0.08404541, -0.023880005, -0.012252808, 0.03778076, -0.029327393, 0.092163086, -0.014587402, 0.009086609, -0.0029392242, -0.07244873, 0.022003174, -0.054901123, 0.020767212, -0.020462036, -0.017486572, 0.029632568, -0.011276245, -0.08093262, 0.012191772, 0.008270264, 0.018463135, 0.019348145, 0.039855957]]\n",
      "Document 0: Fof\n",
      "(r2L)(\u000b\u00121;\u000b\u00001\u00122)\n",
      "=\u0014\u000b\u00001In1 0\n",
      "0\u000bIn2\u0015\n",
      "(r2L)(\u00121;\u00122)\u0014\u000b\u00001In1 0\n",
      "0\u000bIn2\u0015\n",
      ":\n",
      "Sharp Minima Can Generalize For Deep Nets\n",
      ".s lower bounded by \u000b\u00002\n",
      "Since all norms are equivalent in ﬁnite dimension, there\n",
      "exists a constant r >0such thatrjjjAjjjF\u0014jjjAjjj2for all\n",
      "symmetric matrices A. So by picking \u000b <pr\n",
      "M, we are\n",
      "guaranteed that\f\f\f\f\f\f(r2L)\u0000\n",
      "T\u000b(\u0012)\u0001\f\f\f\f\f\f\n",
      "2\u0015M.\n",
      "Any minimum with non-zero Hessian will be observationally equivalent to a minimum whose Hessian has an arbitrarily large spectral norm. Therefore for any minimum\n",
      "in the loss function, if there exists another minimum that\n",
      "generalizes better then there exists another minimum that\n",
      "generalizes better and is also sharper according the spectral\n",
      "norm of the Hessian. The spectral norm of critical points’\n",
      "Hessian becomes as a result less relevant as a measure of\n",
      "potential generalization error. Moreover, since the spectral\n",
      "norm lower bounds the trace for a positive semi-deﬁnite\n",
      "\n",
      "Document 1: \u001bl\n",
      "i=r\n",
      "E\n",
      "x\u0018P(x)h\u0000\n",
      "al\n",
      "i\u0000\u0016l\n",
      "i\u00012i\n",
      "(2)\n",
      "where \u0016al\n",
      "iis normalized summed inputs to the ithhidden unit in the lthlayer andgiis a gain parameter scaling the normalized activation before the non-linear activation function. Note the expectation\n",
      "is under the whole training data distribution. It is typically impractical to compute the expectations\n",
      "in Eq. (2) exactly, since it would require forward passes through the whole training dataset with the\n",
      "current set of weights. Instead, \u0016and\u001bare estimated using the empirical samples from the current\n",
      "mini-batch. This puts constraints on the size of a mini-batch and it is hard to apply to recurrent\n",
      "neural networks.\n",
      "3 Layer normalization\n",
      "We now consider the layer normalization method which is designed to overcome the drawbacks of\n",
      "batch normalization.\n",
      "Notice that changes in the output of one layer will tend to cause highly correlated changes in the\n",
      "summed inputs to the next layer, especially with ReLU units whose outputs can change by a lot.\n",
      "This suggests the “covariate shift” problem can be reduced by ﬁxing the mean and the variance of\n",
      "the summed inputs within each layer. We, thus, compute the layer normalization statistics over all\n",
      "the hidden units in the same layer as follows:\n",
      "\u0016l=1\n",
      "\n",
      "Document 2: (rL)\u0000\n",
      "T\u000b(\u0012)\u0001\n",
      "=(rL)(\u0012)D\u000b\n",
      "(r2L)\u0000\n",
      "T\u000b(\u0012)\u0001\n",
      "=D\u000b(r2L)(\u0012)D\u000b:\n",
      "We will show to which extent you can increase several\n",
      "eigenvalues of (r2L)\u0000\n",
      "T\u000b(\u0012)\u0001\n",
      "by varying\u000b.\n",
      "Deﬁnition 6. For eachn\u0002nmatrixA, we deﬁne the vector\n",
      "\u0015(A)of sorted singular values of Awith their multiplicity\n",
      "\u00151(A)\u0015\u00152(A)\u0015\u0001\u0001\u0001\u0015\u0015n(A).\n",
      "IfAis symmetric positive semi-deﬁnite, \u0015(A)is also the\n",
      "vector of its sorted eigenvalues.\n",
      "Theorem 5. For a (K\u00001)-hidden layer rectiﬁed neural\n",
      "network of the form\n",
      "y=\u001erect(\u001erect(\u0001\u0001\u0001\u001erect(x\u0001\u00121)\u0001\u0001\u0001)\u0001\u0012K\u00001)\u0001\u0012K;\n",
      "and critical point \u0012= (\u0012k)k\u0014Kbeing a minimum for L,\n",
      "such that (r2L)(\u0012)has rankr=rank\u0000\n",
      "(r2L)(\u0012)\u0001\n",
      ",8M >0;9\u000b > 0such that\u0010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"can you explain why we would want to do rlhf?\"\n",
    "docs = get_docs(query, top_k=25)\n",
    "\n",
    "\"\"\"\n",
    "docs format:\n",
    "{'docs content 0': 0, 'docs content 1': 1, 'docs content 2': 2}\n",
    "\"\"\"\n",
    "# Limit the number of documents to show to 3\n",
    "for doc, i in list(docs.items())[:3]:\n",
    "    print(f\"Document {i}: {doc}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2doc = {docs[doc]: doc for doc in docs.keys()}\n",
    "i2doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerank with Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "# instantiate the Cohere client\n",
    "co = cohere.Client(api_key=os.environ.get(\"COHERE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/rerank \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0: Index 10, Relevance Score 0.9916195\n",
      "Document 1: Index 20, Relevance Score 0.9553191\n",
      "Document 2: Index 16, Relevance Score 0.9465967\n"
     ]
    }
   ],
   "source": [
    "rerank_docs = co.rerank(\n",
    "    query=query, documents=list(docs.keys()), top_n=3\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "rerank_docs format:\n",
    "RerankResponse(id='8476ace7-df8b-4075-b1c7-50aa8ba4f7b4', results=[RerankResponseResultsItem(document=None, index=10, relevance_score=0.9916195), RerankResponseResultsItem(document=None, index=20, relevance_score=0.9553191), RerankResponseResultsItem(document=None, index=16, relevance_score=0.9465967)], meta=ApiMeta(api_version=ApiMetaApiVersion(version='1', is_deprecated=None, is_experimental=None), billed_units=ApiMetaBilledUnits(input_tokens=None, output_tokens=None, search_units=1.0, classifications=None), warnings=None))\n",
    "\"\"\"\n",
    "# List the top 3 documents with only its index and relevance score\n",
    "for i, doc in enumerate(rerank_docs.results):\n",
    "    print(f\"Document {i}: Index {doc.index}, Relevance Score {doc.relevance_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
