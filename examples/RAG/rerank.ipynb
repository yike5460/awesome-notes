{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU datasets==2.14.5 openai==1.14.3 pinecone-client==3.2.2 cohere==5.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading data: 100%|██████████| 153M/153M [00:02<00:00, 66.2MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:02<00:00,  2.60s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1134.21it/s]\n",
      "Generating train split: 41584 examples [00:00, 121264.58 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 41584\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"jamescalam/ai-arxiv-chunked\", split=\"train\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '1910.01108',\n",
       " 'chunk-id': '0',\n",
       " 'chunk': 'DistilBERT, a distilled version of BERT: smaller,\\nfaster, cheaper and lighter\\nVictor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\\nHugging Face\\n{victor,lysandre,julien,thomas}@huggingface.co\\nAbstract\\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent\\nin Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains\\nchallenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be ﬁnetuned with good performances on a wide range of tasks like its larger counterparts.\\nWhile most prior work investigated the use of distillation for building task-speciﬁc\\nmodels, we leverage knowledge distillation during the pre-training phase and show\\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\\nof its language understanding capabilities and being 60% faster. To leverage the\\ninductive biases learned by larger models during pre-training, we introduce a triple\\nloss combining language modeling, distillation and cosine-distance losses. Our\\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its',\n",
       " 'id': '1910.01108',\n",
       " 'title': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter',\n",
       " 'summary': 'As Transfer Learning from large-scale pre-trained models becomes more\\nprevalent in Natural Language Processing (NLP), operating these large models in\\non-the-edge and/or under constrained computational training or inference\\nbudgets remains challenging. In this work, we propose a method to pre-train a\\nsmaller general-purpose language representation model, called DistilBERT, which\\ncan then be fine-tuned with good performances on a wide range of tasks like its\\nlarger counterparts. While most prior work investigated the use of distillation\\nfor building task-specific models, we leverage knowledge distillation during\\nthe pre-training phase and show that it is possible to reduce the size of a\\nBERT model by 40%, while retaining 97% of its language understanding\\ncapabilities and being 60% faster. To leverage the inductive biases learned by\\nlarger models during pre-training, we introduce a triple loss combining\\nlanguage modeling, distillation and cosine-distance losses. Our smaller, faster\\nand lighter model is cheaper to pre-train and we demonstrate its capabilities\\nfor on-device computations in a proof-of-concept experiment and a comparative\\non-device study.',\n",
       " 'source': 'http://arxiv.org/pdf/1910.01108',\n",
       " 'authors': ['Victor Sanh',\n",
       "  'Lysandre Debut',\n",
       "  'Julien Chaumond',\n",
       "  'Thomas Wolf'],\n",
       " 'categories': ['cs.CL'],\n",
       " 'comment': 'February 2020 - Revision: fix bug in evaluation metrics, updated\\n  metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at\\n  the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing\\n  - NeurIPS 2019',\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.CL',\n",
       " 'published': '20191002',\n",
       " 'updated': '20200301',\n",
       " 'references': [{'id': '1910.01108'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '1910.01108',\n",
       " 'chunk-id': '1',\n",
       " 'chunk': 'loss combining language modeling, distillation and cosine-distance losses. Our\\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its\\ncapabilities for on-device computations in a proof-of-concept experiment and a\\ncomparative on-device study.\\n1 Introduction\\nFigure 1: Parameter counts of several recently released\\npretrained language models.The last two years have seen the rise\\nof Transfer Learning approaches in\\nNatural Language Processing (NLP)\\nwith large-scale pre-trained language\\nmodels becoming a basic tool in\\nmany NLP tasks [Devlin et al., 2018,\\nRadford et al., 2019, Liu et al., 2019].\\nWhile these models lead to signiﬁcant improvement, they often have\\nseveral hundred million parameters\\nand current research1on pre-trained\\nmodels indicates that training even\\nlarger models still leads to better performances on downstream tasks.\\nThe trend toward bigger models\\nraises several concerns. First is the\\nenvironmental cost of exponentially scaling these models’ computational requirements as mentioned\\nin Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device\\nin real-time has the potential to enable novel and interesting language processing applications, the\\ngrowing computational and memory requirements of these models may hamper wide adoption.',\n",
       " 'id': '1910.01108',\n",
       " 'title': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter',\n",
       " 'summary': 'As Transfer Learning from large-scale pre-trained models becomes more\\nprevalent in Natural Language Processing (NLP), operating these large models in\\non-the-edge and/or under constrained computational training or inference\\nbudgets remains challenging. In this work, we propose a method to pre-train a\\nsmaller general-purpose language representation model, called DistilBERT, which\\ncan then be fine-tuned with good performances on a wide range of tasks like its\\nlarger counterparts. While most prior work investigated the use of distillation\\nfor building task-specific models, we leverage knowledge distillation during\\nthe pre-training phase and show that it is possible to reduce the size of a\\nBERT model by 40%, while retaining 97% of its language understanding\\ncapabilities and being 60% faster. To leverage the inductive biases learned by\\nlarger models during pre-training, we introduce a triple loss combining\\nlanguage modeling, distillation and cosine-distance losses. Our smaller, faster\\nand lighter model is cheaper to pre-train and we demonstrate its capabilities\\nfor on-device computations in a proof-of-concept experiment and a comparative\\non-device study.',\n",
       " 'source': 'http://arxiv.org/pdf/1910.01108',\n",
       " 'authors': ['Victor Sanh',\n",
       "  'Lysandre Debut',\n",
       "  'Julien Chaumond',\n",
       "  'Thomas Wolf'],\n",
       " 'categories': ['cs.CL'],\n",
       " 'comment': 'February 2020 - Revision: fix bug in evaluation metrics, updated\\n  metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at\\n  the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing\\n  - NeurIPS 2019',\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.CL',\n",
       " 'published': '20191002',\n",
       " 'updated': '20200301',\n",
       " 'references': [{'id': '1910.01108'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 41584/41584 [00:07<00:00, 5689.43 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'metadata'],\n",
       "    num_rows: 41584\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.map(lambda x: {\n",
    "    \"id\": f'{x[\"id\"]}-{x[\"chunk-id\"]}',\n",
    "    \"text\": x[\"chunk\"],\n",
    "    \"metadata\": {\n",
    "        \"title\": x[\"title\"],\n",
    "        \"url\": x[\"source\"],\n",
    "        \"primary_category\": x[\"primary_category\"],\n",
    "        \"published\": x[\"published\"],\n",
    "        \"updated\": x[\"updated\"],\n",
    "        \"text\": x[\"chunk\"],\n",
    "    }\n",
    "})\n",
    "# drop uneeded columns\n",
    "data = data.remove_columns([\n",
    "    \"title\", \"summary\", \"source\",\n",
    "    \"authors\", \"categories\", \"comment\",\n",
    "    \"journal_ref\", \"primary_category\",\n",
    "    \"published\", \"updated\", \"references\",\n",
    "    \"doi\", \"chunk-id\",\n",
    "    \"chunk\"\n",
    "])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "embed_model = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from pinecone import Pinecone, ServerlessSpec\n",
    "from pinecone import Pinecone, PodSpec\n",
    "pc = Pinecone(\n",
    "    api_key=os.environ.get(\"PINECONE_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"starter-index\"\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pc.list_indexes():\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=PodSpec(\n",
    "            environment=\"gcp-starter\"\n",
    "        )\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.data.index.Index at 0x7fdbd2a226e0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-large\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "get_embedding(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 100  # how many embeddings we create and insert at once\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    passed = False\n",
    "    # find end of batch\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    # create batch\n",
    "    batch = data[i:i_end]\n",
    "    # create embeddings (exponential backoff to avoid RateLimitError)\n",
    "    for j in range(5):  # max 5 retries\n",
    "        try:\n",
    "            res = client.embeddings.create(input=batch[\"text\"], model=embed_model)\n",
    "            print(f\"Created embeddings for batch {i}-{i_end}.\")\n",
    "            passed = True\n",
    "        except Exception as e:\n",
    "            time.sleep(2**j)  # wait 2^j seconds before retrying\n",
    "            print(\"Retrying...\")\n",
    "    if not passed:\n",
    "        raise RuntimeError(\"Failed to create embeddings.\")\n",
    "    # get embeddings\n",
    "    embeds = [record['embedding'] for record in res['data']]\n",
    "    to_upsert = list(zip(batch[\"id\"], embeds, batch[\"metadata\"]))\n",
    "    # upsert to Pinecone\n",
    "    index.upsert(vectors=to_upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(query: str, top_k: int):\n",
    "    # encode query\n",
    "    xq = get_embedding(query)\n",
    "    # search pinecone index\n",
    "    res = index.query(xq, top_k=top_k, include_metadata=True)\n",
    "    # get doc text\n",
    "    docs = {x[\"metadata\"]['text']: i for i, x in enumerate(res[\"matches\"])}\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"can you explain why we would want to do rlhf?\"\n",
    "docs = get_docs(query, top_k=25)\n",
    "print(\"\\n---\\n\".join(docs.keys()[:3]))  # print the first 3 docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerank with Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "# instantiate the Cohere client\n",
    "co = cohere.Client(api_key=os.environ.get(\"COHERE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
