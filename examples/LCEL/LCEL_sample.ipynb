{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Official documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick tuturial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# read from the env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why don't ice creams ever get invited to parties?\\n\\nBecause they always bring the scoop!\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()} |\n",
    "    prompt |\n",
    "    model |\n",
    "    output_parser\n",
    ")\n",
    "\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object RunnableSequence.ainvoke at 0x7f50b94f8eb0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asychronous\n",
    "chain.ainvoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Why don't ice creams ever get into arguments?\\n\\nBecause they always stay cool!\",\n",
       " \"Why don't spaghetti like to play hide and seek?\\n\\nBecause they always pasta their hiding place!\",\n",
       " \"Why don't dumplings ever play hide and seek?\\n\\nBecause they always end up in hot water!\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch\n",
    "chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't ice creams ever get into arguments?\n",
      "\n",
      "Because they always stay cool!"
     ]
    }
   ],
   "source": [
    "# stream\n",
    "chain.stream(\"ice cream\")\n",
    "for chunk in chain.stream(\"ice cream\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the internal workflow flow (prompt->model->output_parser) when chain.invoke({\"topic\": \"ice cream\"}) is called:\n",
    "\n",
    "prompt: BasePromptTemplate.invoke(input, config): This method is called first, with input being {\"topic\": \"ice cream\"}. It formats the prompt using the input variables and returns a PromptValue object. This object represents a filled prompt that's ready to be passed to a language model.\n",
    "\n",
    "model: BaseChatModel.invoke(input, config, **kwargs): This method is called next, with input being the PromptValue object returned by the previous step. It converts the PromptValue object into a format that's suitable for the language model, generates a response using the language model, and returns a BaseMessage object. This object represents the response generated by the language model.\n",
    "\n",
    "output_parser: BaseGenerationOutputParser.invoke(input, config): This method is called last, with input being the BaseMessage object returned by the previous step. It parses the BaseMessage object and returns the parsed result.\n",
    "\n",
    "Each invoke method is responsible for handling one step in the pipeline, and the output of each step is passed as the input to the next step. This allows for a clear separation of responsibilities and makes it easy to modify or replace individual steps in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# [MermaidChart: 15383246-018f-44ee-9732-71ffd9e6c924]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_value:  messages=[HumanMessage(content='tell me a short joke about ice cream')]\n",
      "prompt_value.to_string():  Human: tell me a short joke about ice cream\n",
      "prompt_value.to_messages():  [HumanMessage(content='tell me a short joke about ice cream')]\n",
      "prompt_value.to_json():  {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptValue'], 'kwargs': {'messages': [HumanMessage(content='tell me a short joke about ice cream')]}}\n"
     ]
    }
   ],
   "source": [
    "prompt_value = prompt.invoke({\"topic\": \"ice cream\"})\n",
    "print('prompt_value: ', prompt_value)\n",
    "print('prompt_value.to_string(): ', prompt_value.to_string())\n",
    "print('prompt_value.to_messages(): ', prompt_value.to_messages())\n",
    "print('prompt_value.to_json(): ', prompt_value.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOpenAI message:  content=\"Why don't ice creams ever get invited to parties?\\n\\nBecause they always drip when they're in the heat!\"\n",
      "OpenAI message:  \n",
      "\n",
      "Robot: Why did the ice cream go to therapy? Because it was having a meltdown!\n"
     ]
    }
   ],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# model = ChatOpenAI(model=\"gpt-4\")\n",
    "message  = model.invoke(prompt_value)\n",
    "print('ChatOpenAI message: ', message)\n",
    "\n",
    "from langchain_openai.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "print('OpenAI message: ', llm.invoke(prompt_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:  Why don't ice creams ever get invited to parties?\n",
      "\n",
      "Because they always drip when they're in the heat!\n"
     ]
    }
   ],
   "source": [
    "result = output_parser.invoke(message)\n",
    "print('result: ', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add with retrieval component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py310/lib/python3.10/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Harrison worked at Kensho.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Requires:\n",
    "# pip install langchain docarray tiktoken\n",
    "# Fix the error in community lib according to PR: https://github.com/langchain-ai/langchain/pull/18678 befor executing this code\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "\n",
    "chain.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='harrison worked at kensho'),\n",
       " Document(page_content='bears like to eat honey')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add configurable component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "configurable_model = model.configurable_alternatives(\n",
    "    ConfigurableField(id=\"model\"), \n",
    "    default_key=\"chat_openai\", \n",
    "    openai=llm,\n",
    "    # anthropic=anthropic,\n",
    ")\n",
    "configurable_chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | configurable_model \n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't ice creams ever get invited to parties?\n",
      "\n",
      "Because they always melt under pressure!"
     ]
    }
   ],
   "source": [
    "stream = configurable_chain.stream(\n",
    "    \"ice cream\", \n",
    "    config={\"model\": \"openai\"}\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full process with fallback && logging component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, ConfigurableField\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "chat_openai = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "openai = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "anthropic = ChatAnthropic(model=\"claude-2\")\n",
    "model = (\n",
    "    chat_openai\n",
    "    .with_fallbacks([anthropic])\n",
    "    .configurable_alternatives(\n",
    "        ConfigurableField(id=\"model\"),\n",
    "        default_key=\"chat_openai\",\n",
    "        openai=openai,\n",
    "        anthropic=anthropic,\n",
    "    )\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to batch ingest runs: LangSmithAuthError('Authentication failed for https://api.smith.langchain.com/runs/batch. HTTPError(\\'401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Invalid API key\"}\\')')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream truck break down? It had too many \"scoops\"!'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to batch ingest runs: LangSmithAuthError('Authentication failed for https://api.smith.langchain.com/runs/batch. HTTPError(\\'401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Invalid API key\"}\\')')\n"
     ]
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The built-in optimizations\n",
    "\n",
    "- Stream: By default, stream runs invoke() in a loop, yielding results.\n",
    "             Override to optimize streaming.\n",
    "\n",
    "- Batch: By default, batch runs invoke() in parallel using a thread pool executor.\n",
    "             Override to optimize batching.\n",
    "\n",
    "- Async: Methods with \"a\" suffix are asynchronous. By default, they execute\n",
    "             the sync counterpart using asyncio's thread pool.\n",
    "             Override for native async."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnable component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'origin': 1, 'modified': 2}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    origin=RunnablePassthrough(),\n",
    "    modified=lambda x: x+1\n",
    ")\n",
    "\n",
    "runnable.invoke(1) # {'origin': 1, 'modified': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LangChain Expression Language (LCEL), RunnableParallel, RunnableLambda, and RunnablePassthrough are key components that facilitate the construction of complex chains for processing data, executing custom functions, and managing data flow. Each serves a distinct purpose and operates differently within the LCEL framework. Understanding their usage and differences is crucial for effectively leveraging LangChain for various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RunnableParallel**\n",
    "\n",
    "RunnableParallel allows for the execution of multiple Runnables in parallel, combining their outputs into a single map. This is particularly useful when you need to perform several operations that do not depend on each other and can be executed simultaneously, thereby reducing the overall execution time. It is ideal for scenarios where you need to fetch or process data from multiple sources concurrently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "# Assuming joke_chain and poem_chain are predefined Runnables\n",
    "joke_chain = RunnableParallel()\n",
    "poem_chain = RunnableParallel()\n",
    "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
    "result = map_chain.invoke({\"topic\": \"bear\"})\n",
    "print(result[\"joke\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RunnableLambda**\n",
    "\n",
    "RunnableLambda turns custom Python functions into Runnables that can be integrated into LCEL chains. This component is essential for incorporating arbitrary logic or transformations that are not directly supported by other Runnables. It accepts a single argument, making it necessary to wrap functions that require multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "def add_one(x):\n",
    "    return x + 1\n",
    "add_one_runnable = RunnableLambda(add_one)\n",
    "result = add_one_runnable.invoke(5)\n",
    "print(result) # 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RunnablePassthrough**\n",
    "\n",
    "RunnablePassthrough is used to pass inputs through unchanged or with the addition of extra keys. It is particularly useful in scenarios where part of the input needs to be directly forwarded to subsequent components in the chain without modification. It can also be used to assign new keys to the data being passed through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num': 1, 'extra': 3}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "runnable = RunnablePassthrough.assign(extra=lambda x: x[\"num\"] * 3)\n",
    "result = runnable.invoke({\"num\": 1})\n",
    "print(result) # 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Differences**\n",
    "\n",
    "- Execution Mode: RunnableParallel executes multiple Runnables in parallel and combines their outputs, whereas RunnableLambda and RunnablePassthrough operate on a single input.\n",
    "- Purpose: RunnableLambda is designed for integrating custom functions into LCEL chains, allowing for arbitrary logic to be executed. In contrast, RunnablePassthrough is used for forwarding inputs unchanged or with minor modifications, and RunnableParallel is for parallel execution of independent tasks.\n",
    "- Input Handling: RunnableLambda requires a single argument for its custom function, which may necessitate wrapping for multi-argument functions. RunnablePassthrough and RunnableParallel deal with inputs in the form of maps, facilitating the handling of multiple data points simultaneously."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
