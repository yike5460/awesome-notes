{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Official documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why don't ice cream cones have secrets?\\n\\nBecause they always spill the scoop!\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the internal workflow flow (prompt->model->output_parser) when chain.invoke({\"topic\": \"ice cream\"}) is called:\n",
    "\n",
    "prompt: BasePromptTemplate.invoke(input, config): This method is called first, with input being {\"topic\": \"ice cream\"}. It formats the prompt using the input variables and returns a PromptValue object. This object represents a filled prompt that's ready to be passed to a language model.\n",
    "\n",
    "model: BaseChatModel.invoke(input, config, **kwargs): This method is called next, with input being the PromptValue object returned by the previous step. It converts the PromptValue object into a format that's suitable for the language model, generates a response using the language model, and returns a BaseMessage object. This object represents the response generated by the language model.\n",
    "\n",
    "output_parser: BaseGenerationOutputParser.invoke(input, config): This method is called last, with input being the BaseMessage object returned by the previous step. It parses the BaseMessage object and returns the parsed result.\n",
    "\n",
    "Each invoke method is responsible for handling one step in the pipeline, and the output of each step is passed as the input to the next step. This allows for a clear separation of responsibilities and makes it easy to modify or replace individual steps in the pipeline.\n",
    "\n",
    "# [MermaidChart: 15383246-018f-44ee-9732-71ffd9e6c924]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_value:  messages=[HumanMessage(content='tell me a short joke about ice cream')]\n",
      "prompt_value.to_string():  Human: tell me a short joke about ice cream\n",
      "prompt_value.to_messages():  [HumanMessage(content='tell me a short joke about ice cream')]\n",
      "prompt_value.to_json():  {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptValue'], 'kwargs': {'messages': [HumanMessage(content='tell me a short joke about ice cream')]}}\n"
     ]
    }
   ],
   "source": [
    "prompt_value = prompt.invoke({\"topic\": \"ice cream\"})\n",
    "print('prompt_value: ', prompt_value)\n",
    "print('prompt_value.to_string(): ', prompt_value.to_string())\n",
    "print('prompt_value.to_messages(): ', prompt_value.to_messages())\n",
    "print('prompt_value.to_json(): ', prompt_value.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOpenAI message:  content=\"Why don't ice creams ever get invited to parties?\\n\\nBecause they always drip when they're in the heat!\"\n",
      "OpenAI message:  \n",
      "\n",
      "Robot: Why did the ice cream go to therapy? Because it was having a meltdown!\n"
     ]
    }
   ],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# model = ChatOpenAI(model=\"gpt-4\")\n",
    "message  = model.invoke(prompt_value)\n",
    "print('ChatOpenAI message: ', message)\n",
    "\n",
    "from langchain_openai.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "print('OpenAI message: ', llm.invoke(prompt_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:  Why don't ice creams ever get invited to parties?\n",
      "\n",
      "Because they always drip when they're in the heat!\n"
     ]
    }
   ],
   "source": [
    "result = output_parser.invoke(message)\n",
    "print('result: ', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py310/lib/python3.10/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Harrison worked at Kensho.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Requires:\n",
    "# pip install langchain docarray tiktoken\n",
    "# Fix the error in community lib according to PR: https://github.com/langchain-ai/langchain/pull/18678 befor executing this code\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "\n",
    "chain.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
