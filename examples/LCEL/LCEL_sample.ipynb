{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Official documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick tuturial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# read from the env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why don't ice creams ever get invited to parties?\\n\\nBecause they always bring the scoop!\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()} |\n",
    "    prompt |\n",
    "    model |\n",
    "    output_parser\n",
    ")\n",
    "\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object RunnableSequence.ainvoke at 0x7f50b94f8eb0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asychronous\n",
    "chain.ainvoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Why don't ice creams ever get into arguments?\\n\\nBecause they always stay cool!\",\n",
       " \"Why don't spaghetti like to play hide and seek?\\n\\nBecause they always pasta their hiding place!\",\n",
       " \"Why don't dumplings ever play hide and seek?\\n\\nBecause they always end up in hot water!\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch\n",
    "chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't ice creams ever get into arguments?\n",
      "\n",
      "Because they always stay cool!"
     ]
    }
   ],
   "source": [
    "# stream\n",
    "chain.stream(\"ice cream\")\n",
    "for chunk in chain.stream(\"ice cream\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the internal workflow flow (prompt->model->output_parser) when chain.invoke({\"topic\": \"ice cream\"}) is called:\n",
    "\n",
    "prompt: BasePromptTemplate.invoke(input, config): This method is called first, with input being {\"topic\": \"ice cream\"}. It formats the prompt using the input variables and returns a PromptValue object. This object represents a filled prompt that's ready to be passed to a language model.\n",
    "\n",
    "model: BaseChatModel.invoke(input, config, **kwargs): This method is called next, with input being the PromptValue object returned by the previous step. It converts the PromptValue object into a format that's suitable for the language model, generates a response using the language model, and returns a BaseMessage object. This object represents the response generated by the language model.\n",
    "\n",
    "output_parser: BaseGenerationOutputParser.invoke(input, config): This method is called last, with input being the BaseMessage object returned by the previous step. It parses the BaseMessage object and returns the parsed result.\n",
    "\n",
    "Each invoke method is responsible for handling one step in the pipeline, and the output of each step is passed as the input to the next step. This allows for a clear separation of responsibilities and makes it easy to modify or replace individual steps in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# [MermaidChart: 15383246-018f-44ee-9732-71ffd9e6c924]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_value:  messages=[HumanMessage(content='tell me a short joke about ice cream')]\n",
      "prompt_value.to_string():  Human: tell me a short joke about ice cream\n",
      "prompt_value.to_messages():  [HumanMessage(content='tell me a short joke about ice cream')]\n",
      "prompt_value.to_json():  {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptValue'], 'kwargs': {'messages': [HumanMessage(content='tell me a short joke about ice cream')]}}\n"
     ]
    }
   ],
   "source": [
    "prompt_value = prompt.invoke({\"topic\": \"ice cream\"})\n",
    "print('prompt_value: ', prompt_value)\n",
    "print('prompt_value.to_string(): ', prompt_value.to_string())\n",
    "print('prompt_value.to_messages(): ', prompt_value.to_messages())\n",
    "print('prompt_value.to_json(): ', prompt_value.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOpenAI message:  content=\"Why don't ice creams ever get invited to parties?\\n\\nBecause they always drip when they're in the heat!\"\n",
      "OpenAI message:  \n",
      "\n",
      "Robot: Why did the ice cream go to therapy? Because it was having a meltdown!\n"
     ]
    }
   ],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# model = ChatOpenAI(model=\"gpt-4\")\n",
    "message  = model.invoke(prompt_value)\n",
    "print('ChatOpenAI message: ', message)\n",
    "\n",
    "from langchain_openai.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "print('OpenAI message: ', llm.invoke(prompt_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:  Why don't ice creams ever get invited to parties?\n",
      "\n",
      "Because they always drip when they're in the heat!\n"
     ]
    }
   ],
   "source": [
    "result = output_parser.invoke(message)\n",
    "print('result: ', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add with retrieval component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py310/lib/python3.10/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Harrison worked at Kensho.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Requires:\n",
    "# pip install langchain docarray tiktoken\n",
    "# Fix the error in community lib according to PR: https://github.com/langchain-ai/langchain/pull/18678 befor executing this code\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "\n",
    "chain.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='harrison worked at kensho'),\n",
       " Document(page_content='bears like to eat honey')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add configurable component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "configurable_model = model.configurable_alternatives(\n",
    "    ConfigurableField(id=\"model\"), \n",
    "    default_key=\"chat_openai\", \n",
    "    openai=llm,\n",
    "    # anthropic=anthropic,\n",
    ")\n",
    "configurable_chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | configurable_model \n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't ice creams ever get invited to parties?\n",
      "\n",
      "Because they always melt under pressure!"
     ]
    }
   ],
   "source": [
    "stream = configurable_chain.stream(\n",
    "    \"ice cream\", \n",
    "    config={\"model\": \"openai\"}\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full process with fallback && logging component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, ConfigurableField\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "chat_openai = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "openai = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "anthropic = ChatAnthropic(model=\"claude-2\")\n",
    "model = (\n",
    "    chat_openai\n",
    "    .with_fallbacks([anthropic])\n",
    "    .configurable_alternatives(\n",
    "        ConfigurableField(id=\"model\"),\n",
    "        default_key=\"chat_openai\",\n",
    "        openai=openai,\n",
    "        anthropic=anthropic,\n",
    "    )\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to batch ingest runs: LangSmithAuthError('Authentication failed for https://api.smith.langchain.com/runs/batch. HTTPError(\\'401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Invalid API key\"}\\')')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream truck break down? It had too many \"scoops\"!'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to batch ingest runs: LangSmithAuthError('Authentication failed for https://api.smith.langchain.com/runs/batch. HTTPError(\\'401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Invalid API key\"}\\')')\n"
     ]
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The built-in optimizations\n",
    "\n",
    "- Stream: By default, stream runs invoke() in a loop, yielding results.\n",
    "             Override to optimize streaming.\n",
    "\n",
    "- Batch: By default, batch runs invoke() in parallel using a thread pool executor.\n",
    "             Override to optimize batching.\n",
    "\n",
    "- Async: Methods with \"a\" suffix are asynchronous. By default, they execute\n",
    "             the sync counterpart using asyncio's thread pool.\n",
    "             Override for native async."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
