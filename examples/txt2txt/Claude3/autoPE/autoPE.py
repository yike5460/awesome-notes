# Check the GitHub repository (https://github.com/aws-samples/claude-prompt-generator) for the latest implementation with Gradio UI
import re
from dotenv import load_dotenv
import csv
import boto3
import json
from termcolor import colored

load_dotenv()

# Initialize the Amazon Bedrock boto3 client
bedrock_runtime = boto3.client(service_name='bedrock-runtime', region_name="us-east-1")

dataset_gen_prompt_template = """
This is the user input prompt: {_prompt}\n\nGenerate {_number} questions a user would ask about this topic. Wrap each question in <case></case> XML tags. Make sure the responses are varied, and cover edge cases and possible issues.
"""

dataset_eval_prompt_template = """
This is the user input question: {_question}\n\nPlease answer the question with concise and accurate information, consider to use bullet points to make the answer more readable and easy to understand, the overall answer should be around 100-200 words, and finally the output should be wrapped in <answer></answer> XML tags.
"""

fix_prompt_template = """
Here is a orginal prompt for an Claude3: {prompt}\n\nI have a few requirement of how Claude3 should responsded to my prompts and the original prompt should be updated accordingly. Here are the evaluations:\n{evaluation_summary}\n\nPlease provide an improved version of the original prompt based on the evaluations. Please analyze the evalution and consider how original prompt can be improved within <thinking></thinking> tags, then respond with the revised prompt in <fixed_prompt></fixed_prompt> tags.
"""

model_id = 'anthropic.claude-3-sonnet-20240229-v1:0'
default_system = "you have profound knowledge and hands on experience in field of software engineering and artificial intelligence, you are also an experienced solution architect in Amazon Web Service and have expertise to impelment model application development with AWS in consdieration of well architect and industry best practice."

def generate_test_dataset(prompt, number=10):
    """
    This function generates a test dataset by invoking a model with a given prompt.

    Parameters:
    prompt (str): The user input prompt.

    Returns:
    matches (list): A list of questions generated by the model, each wrapped in <case></case> XML tags.
    """
    message = {
        "role": "user",
        "content": [
            # {"type": "image", "source": {"type": "base64", "media_type": "image/jpeg", "data": content_image}},
            {"type": "text", "text": dataset_gen_prompt_template.format(_prompt=prompt, _number=number)}
        ]
    }
    messages = [message]
    body = json.dumps({
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": 4000,
        "messages": messages,
        "system": default_system,
    })
    response = bedrock_runtime.invoke_model(body=body, modelId=model_id)
    response_body = json.loads(response.get('body').read())
    pattern = r'<case>(.*?)</case>'
    matches = re.findall(pattern, response_body['content'][0]['text'], re.DOTALL)
    return matches

def test_prompt(prompt, test_cases):
    """
    This function tests the given prompt on the generated test cases. It invokes a model with each test case and prints the model's response.

    Parameters:
    prompt (str): The user input prompt.
    test_cases (list): A list of test cases to test the prompt on.

    Returns:
    results (list): A list of dictionaries containing the test case, model response, and evaluation.
    """
    results = []
    for case in test_cases:
        message = {
            "role": "user",
            "content": [
                {"type": "text", "text": dataset_eval_prompt_template.format(_question=prompt)}
            ]
        }
        messages = [message]
        body = json.dumps({
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 4000,
            "messages": messages,
            "system": default_system,
        })
        response = bedrock_runtime.invoke_model(body=body, modelId=model_id)
        response_body = json.loads(response.get('body').read())
        answer = response_body['content'][0]['text']
        print(colored(f"Test Case:\n {case}", "green"))
        print(colored(f"Answer\n: {answer}", "yellow"))
        evaluation = input(colored("Enter your evaluation: ", "red"))
        if evaluation == "":
            evaluation = "No evaluation provided."
        if evaluation == "exit":
            break
        results.append({"Test Case": case, "Answer": answer, "Evaluation": evaluation})
    return results

def save_results_to_csv(results, filename):
    """Save the test results to a CSV file."""
    fieldnames = ["Test Case", "Answer", "Evaluation"]
    with open(filename, "w", newline="") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(results)

def generate_revised_prompt(prompt, evaluations):
    """Generate an improved prompt using the LLM."""
    evaluation_summary = "\n".join(evaluations)
    message = {
        "role": "user",
        "content": [
            {"type": "text", "text": fix_prompt_template.format(prompt=prompt, evaluation_summary=evaluation_summary)}
        ]
    }
    messages = [message]
    body = json.dumps({
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": 4000,
        "messages": messages,
        "system": default_system,
    })
    response = bedrock_runtime.invoke_model(body=body, modelId=model_id)
    response_body = json.loads(response.get('body').read())
    pattern = r'<fixed_prompt>(.*?)</fixed_prompt>'
    matches = re.findall(pattern, response_body['content'][0]['text'], re.DOTALL)
    return matches[0]

def main():
    initial_prompt = input("Enter the initial prompt: ")
    test_cases = generate_test_dataset(initial_prompt, number=3)
    print("\nGenerated Test Cases:")
    for case in test_cases:
        print(colored(case, "blue"))
    print("\nTesting Prompt...")
    results = test_prompt(initial_prompt, test_cases)
    print("\nTest Results:")
    for result in results:
        print(colored(result["Test Case"], "blue"))
        print(colored(result["Answer"], "yellow"))
        print(colored(result["Evaluation"], "red") + "\n")
    save_results_to_csv(results, "test_results.csv")
    print("\nTest results saved to test_results.csv")
    evaluations = [result["Evaluation"] for result in results]
    revised_prompt = generate_revised_prompt(initial_prompt, evaluations)
    print("\nRevised Prompt:")
    print(colored(revised_prompt, "green"))

if __name__ == "__main__":
    main()