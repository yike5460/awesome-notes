{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes-cuda112\n",
    "!pip install accelerate\n",
    "!pip install xformers==0.0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 3 µs, total: 5 µs\n",
      "Wall time: 8.34 µs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"温故而知新\"是一个中国古代成语，出自《论语·为政》。它的意思是通过回顾和了解过去的事情，可以发现新的知识和道理。这个成语强调了学习和思考的重要性，鼓励人们在不断积累知识的过程中，不断地总结经验教训，从而实现自我提升和成长。\n"
     ]
    }
   ],
   "source": [
    "# Running without inference acceleration\n",
    "%time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan2-7B-Chat\", use_fast=False, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan2-7B-Chat\", device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "model.generation_config = GenerationConfig.from_pretrained(\"baichuan-inc/Baichuan2-7B-Chat\")\n",
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": \"解释一下“温故而知新”\"})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Host Baichuan model on Amazon SageMaker using LMI(vLLM DeepSpeed) container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sagemaker --upgrade  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py310/lib/python3.10/site-packages/sagemaker/base_serializers.py:28: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 1.21.5)\n",
      "  import scipy.sparse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ubuntu/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "import io\n",
    "import numpy as np\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "session = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = session._region_name  # region name of the current SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile serving.properties\n",
    "engine=MPI\n",
    "option.model_id=baichuan-inc/Baichuan2-7B-Chat\n",
    "option.tensor_parallel_degree=max\n",
    "option.rolling_batch=auto\n",
    "option.max_rolling_batch_size=32\n",
    "option.model_loading_timeout=7200\n",
    "option.trust_remote_code=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baichuan2-7B-Chat-model/\n",
      "Baichuan2-7B-Chat-model/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "rm -rf Baichuan2-7B-Chat-model.tar.gz\n",
    "mkdir Baichuan2-7B-Chat-model\n",
    "mv serving.properties Baichuan2-7B-Chat-model/\n",
    "tar czvf Baichuan2-7B-Chat-model.tar.gz Baichuan2-7B-Chat-model/\n",
    "rm -rf Baichuan2-7B-Chat-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.26.0-tensorrtllm0.7.1-cu122'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Refer to https://github.com/aws/deep-learning-containers/blob/master/available_images.md for all available images\n",
    "\n",
    "\"\"\"\n",
    "all available framework under library folder sagemaker/image_uri_config\n",
    "autogluon.json                      knn.json\n",
    "blazingtext.json                    lda.json\n",
    "chainer.json                        linear-learner.json\n",
    "clarify.json                        model-monitor.json\n",
    "coach-mxnet.json                    mxnet.json\n",
    "coach-tensorflow.json               neo-mxnet.json\n",
    "data-wrangler.json                  neo-pytorch.json\n",
    "debugger.json                       neo-tensorflow.json\n",
    "detailed-profiler.json              ntm.json\n",
    "djl-deepspeed.json                  object-detection.json\n",
    "djl-fastertransformer.json          object2vec.json\n",
    "djl-lmi.json                        pca.json\n",
    "djl-neuronx.json                    pytorch-neuron.json\n",
    "djl-tensorrtllm.json                pytorch-smp.json\n",
    "factorization-machines.json         pytorch-training-compiler.json\n",
    "forecasting-deepar.json             pytorch.json\n",
    "huggingface-llm-neuronx.json        randomcutforest.json\n",
    "huggingface-llm.json                ray-pytorch.json\n",
    "huggingface-neuron.json             ray-tensorflow.json\n",
    "huggingface-neuronx.json            sagemaker-base-python.json\n",
    "huggingface-tei-cpu.json            sagemaker-geospatial.json\n",
    "huggingface-tei.json                sagemaker-tritonserver.json\n",
    "huggingface-training-compiler.json  semantic-segmentation.json\n",
    "huggingface.json                    seq2seq.json\n",
    "image-classification-neo.json       sklearn.json\n",
    "image-classification.json           spark.json\n",
    "inferentia-mxnet.json               sparkml-serving.json\n",
    "inferentia-pytorch.json             stabilityai.json\n",
    "inferentia-tensorflow.json          tensorflow.json\n",
    "instance_gpu_info.json              vw.json\n",
    "ipinsights.json                     xgboost-neo.json\n",
    "kmeans.json                         xgboost.json\n",
    "\"\"\"\n",
    "image_uri = image_uris.retrieve(\n",
    "        # There are issue in using deepspeed since the option \"trust_remote_code\" is not supported, some of the issue been raised: https://github.com/aws/sagemaker-python-sdk/issues/4063\n",
    "        framework=\"djl-tensorrtllm\",\n",
    "        region=session.boto_session.region_name,\n",
    "        version=\"0.26.0\"\n",
    "    )\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-east-1-705247044519/large-model-lmi/code/Baichuan2-7B-Chat-model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_prefix = \"large-model-lmi/code\"\n",
    "bucket = session.default_bucket()  # bucket to house artifacts\n",
    "code_artifact = session.upload_data(\"Baichuan2-7B-Chat-model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-10 08:19:15        307 Baichuan2-7B-Chat-model.tar.gz\n",
      "Wed Jul 10 08:19:17 UTC 2024\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://sagemaker-us-east-1-705247044519/large-model-lmi/code/Baichuan2-7B-Chat-model.tar.gz\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" List all the available instance types with GPU support\n",
    "aws ec2 describe-instance-types \\\n",
    "  --query 'InstanceTypes[?GpuInfo.Gpus[0].Name!=`null`] | sort_by(@, &GpuInfo.Gpus[0].Name) | sort_by(@, &to_number(GpuInfo.Gpus[0].MemoryInfo.SizeInMiB)) | reverse(@) | [].{GPUCount:GpuInfo.Gpus[0].Count,GPUMemorySizeInMiB:GpuInfo.Gpus[0].MemoryInfo.SizeInMiB,GPUType:GpuInfo.Gpus[0].Name,InstanceType:InstanceType}' \\\n",
    "  --region us-east-1 \\\n",
    "  --output table\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "|                         DescribeInstanceTypes                        |\n",
    "+----------+----------------------+-------------------+----------------+\n",
    "| GPUCount | GPUMemorySizeInMiB   |      GPUType      | InstanceType   |\n",
    "+----------+----------------------+-------------------+----------------+\n",
    "|  8       |  183105              |  L4               |  g6.48xlarge   |\n",
    "|  4       |  91553               |  L4               |  g6.12xlarge   |\n",
    "|  4       |  91553               |  L4               |  g6.24xlarge   |\n",
    "|  8       |  81920               |  H100             |  p5.48xlarge   |\n",
    "|  8       |  40960               |  A100             |  p4d.24xlarge  |\n",
    "|  8       |  32768               |  V100             |  p3dn.24xlarge |\n",
    "|  8       |  32768               |  Gaudi HL-205     |  dl1.24xlarge  |\n",
    "|  8       |  24576               |  A10G             |  g5.48xlarge   |\n",
    "|  1       |  24576               |  A10G             |  g5.8xlarge    |\n",
    "|  1       |  24576               |  A10G             |  g5.xlarge     |\n",
    "|  1       |  24576               |  A10G             |  g5.16xlarge   |\n",
    "|  4       |  24576               |  A10G             |  g5.12xlarge   |\n",
    "|  4       |  24576               |  A10G             |  g5.24xlarge   |\n",
    "|  1       |  24576               |  A10G             |  g5.2xlarge    |\n",
    "|  1       |  24576               |  A10G             |  g5.4xlarge    |\n",
    "|  1       |  22888               |  L4               |  gr6.4xlarge   |\n",
    "|  1       |  22888               |  L4               |  g6.4xlarge    |\n",
    "|  1       |  22888               |  L4               |  gr6.8xlarge   |\n",
    "|  1       |  22888               |  L4               |  g6.16xlarge   |\n",
    "|  1       |  22888               |  L4               |  g6.xlarge     |\n",
    "|  1       |  22888               |  L4               |  g6.2xlarge    |\n",
    "|  1       |  22888               |  L4               |  g6.8xlarge    |\n",
    "|  4       |  16384               |  V100             |  p3.8xlarge    |\n",
    "|  8       |  16384               |  V100             |  p3.16xlarge   |\n",
    "|  1       |  16384               |  V100             |  p3.2xlarge    |\n",
    "|  1       |  16384               |  T4g              |  g5g.4xlarge   |\n",
    "|  1       |  16384               |  T4g              |  g5g.xlarge    |\n",
    "|  1       |  16384               |  T4g              |  g5g.8xlarge   |\n",
    "|  2       |  16384               |  T4g              |  g5g.metal     |\n",
    "|  2       |  16384               |  T4g              |  g5g.16xlarge  |\n",
    "|  1       |  16384               |  T4g              |  g5g.2xlarge   |\n",
    "|  1       |  16384               |  T4               |  g4dn.4xlarge  |\n",
    "|  1       |  16384               |  T4               |  g4dn.8xlarge  |\n",
    "|  1       |  16384               |  T4               |  g4dn.2xlarge  |\n",
    "|  1       |  16384               |  T4               |  g4dn.16xlarge |\n",
    "|  1       |  16384               |  T4               |  g4dn.xlarge   |\n",
    "|  4       |  16384               |  T4               |  g4dn.12xlarge |\n",
    "|  8       |  16384               |  T4               |  g4dn.metal    |\n",
    "|  1       |  12288               |  K80              |  p2.xlarge     |\n",
    "|  16      |  12288               |  K80              |  p2.16xlarge   |\n",
    "|  8       |  12288               |  K80              |  p2.8xlarge    |\n",
    "|  1       |  8192                |  Radeon Pro V520  |  g4ad.xlarge   |\n",
    "|  4       |  8192                |  Radeon Pro V520  |  g4ad.16xlarge |\n",
    "|  1       |  8192                |  Radeon Pro V520  |  g4ad.2xlarge  |\n",
    "|  2       |  8192                |  Radeon Pro V520  |  g4ad.8xlarge  |\n",
    "|  1       |  8192                |  Radeon Pro V520  |  g4ad.4xlarge  |\n",
    "|  1       |  8192                |  M60              |  g3s.xlarge    |\n",
    "|  1       |  8192                |  M60              |  g3.4xlarge    |\n",
    "|  2       |  8192                |  M60              |  g3.8xlarge    |\n",
    "|  4       |  8192                |  M60              |  g3.16xlarge   |\n",
    "+----------+----------------------+-------------------+----------------+\n",
    "\n",
    "Refer to https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-instance-types.html would be more accurate\n",
    "aws --region us-east-1 pricing get-products \\\n",
    "  --service-code AmazonSageMaker \\\n",
    "  --filters Type=TERM_MATCH,Field=regionCode,Value=us-east-1 \\\n",
    "  | jq -r '\n",
    "    .PriceList[]\n",
    "    | fromjson\n",
    "    | select(.product.productFamily == \"ML Instance\")\n",
    "    | {\n",
    "        GPUCount: .product.attributes.gpu,\n",
    "        GPUMemory: .product.attributes.gpuMemory,\n",
    "        GPUType: .product.attributes.physicalGpu,\n",
    "        InstanceType: .product.attributes.instanceName\n",
    "      }\n",
    "    | select(.GPUType != \"None\" and .GPUType != \"N/A\")\n",
    "    | .GPUMemory = if .GPUMemory != \"N/A\" and .GPUMemory != null then (.GPUMemory | gsub(\"[^0-9]\"; \"\") | tonumber | tostring + \" GiB\") else .GPUMemory end\n",
    "    | [.GPUCount, .GPUMemory, .GPUType, .InstanceType]\n",
    "    | @tsv\n",
    "  ' | column -t -s $'\\t' | sort -k2 -n -r | uniq | echo -e \"GPUCount\\tGPUMemorySizeInGiB\\tGPUType\\tInstanceType\\n$(cat -)\"\n",
    "    \n",
    "GPUCount        GPUMemorySizeInGiB      GPUType InstanceType\n",
    "8    6403 GiB  nvidia h100              ml.p5.48xlarge\n",
    "8    6402 GiB  A100 80GB SXM            ml.p4de.24xlarge\n",
    "8    3202 GiB  nvidia a100 80gb         ml.p4de.24xlarge\n",
    "8    3202 GiB  nvidia a100 40gb         ml.p4d.24xlarge\n",
    "8    3202 GiB  NVIDIA A100 Tensor Core  ml.p4d.24xlarge\n",
    "8    256 GiB   NVIDIA Tesla V100 GPU    ml.p3dn.24xlarge\n",
    "8    192 GiB   nvidia a10g              ml.g5.48xlarge\n",
    "8    192 GiB   NVIDIA L4                ml.g6.48xlarge\n",
    "16   192 GiB   NVIDIA K80 GPU           ml.p2.16xlarge\n",
    "8    128 GiB   NVIDIA Tesla V100 GPU    ml.p3.16xlarge\n",
    "8    96 GiB    NVIDIA K80 GPU           ml.p2.8xlarge\n",
    "4    96 GiB    nvidia a10g              ml.g5.24xlarge\n",
    "4    96 GiB    nvidia a10g              ml.g5.12xlarge\n",
    "4    96 GiB    NVIDIA L4                ml.g6.24xlarge\n",
    "4    96 GiB    NVIDIA L4                ml.g6.12xlarge\n",
    "4    64 GiB    NVIDIA Tesla V100 GPU    ml.p3.8xlarge\n",
    "4    64 GiB    NVIDIA T4 GPU            ml.g4dn.12xlarge\n",
    "1    24 GiB    nvidia a10g              ml.g5.xlarge\n",
    "1    24 GiB    nvidia a10g              ml.g5.8xlarge\n",
    "1    24 GiB    nvidia a10g              ml.g5.4xlarge\n",
    "1    24 GiB    nvidia a10g              ml.g5.2xlarge\n",
    "1    24 GiB    nvidia a10g              ml.g5.16xlarge\n",
    "1    24 GiB    NVIDIA L4                ml.g6.xlarge\n",
    "1    24 GiB    NVIDIA L4                ml.g6.8xlarge\n",
    "1    24 GiB    NVIDIA L4                ml.g6.4xlarge\n",
    "1    24 GiB    NVIDIA L4                ml.g6.2xlarge\n",
    "1    24 GiB    NVIDIA L4                ml.g6.16xlarge\n",
    "1    16 GiB    NVIDIA Tesla V100 GPU    ml.p3.2xlarge\n",
    "1    16 GiB    NVIDIA T4 GPU            ml.g4dn.xlarge\n",
    "1    16 GiB    NVIDIA T4 GPU            ml.g4dn.8xlarge\n",
    "1    16 GiB    NVIDIA T4 GPU            ml.g4dn.4xlarge\n",
    "1    16 GiB    NVIDIA T4 GPU            ml.g4dn.2xlarge\n",
    "1    16 GiB    NVIDIA T4 GPU            ml.g4dn.16xlarge\n",
    "1    12 GiB    NVIDIA K80 GPU           ml.p2.xlarge\n",
    "N/A  N/A       AWS Inferentia2          ml.inf2.xlarge\n",
    "N/A  N/A       AWS Inferentia2          ml.inf2.8xlarge\n",
    "N/A  N/A       AWS Inferentia2          ml.inf2.48xlarge\n",
    "N/A  N/A       AWS Inferentia2          ml.inf2.24xlarge\n",
    "N/A  N/A       AWS Inferentia           ml.inf1.xlarge\n",
    "N/A  N/A       AWS Inferentia           ml.inf1.6xlarge\n",
    "N/A  N/A       AWS Inferentia           ml.inf1.2xlarge\n",
    "N/A  N/A       AWS Inferentia           ml.inf1.24xlarge\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------!"
     ]
    }
   ],
   "source": [
    "model = Model(image_uri=image_uri, model_data=code_artifact, role=role)\n",
    "\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"Baichuan2-7B-Chat-lmi-model\")\n",
    "\n",
    "model.deploy(initial_instance_count=1,\n",
    "             instance_type=instance_type,\n",
    "             endpoint_name=endpoint_name,\n",
    "            #  volume_size =30, ClientError: An error occurred (ValidationException) when calling the CreateEndpointConfig operation: VolumeSize parameter is not allowed for the selected Instance type ml.g4dn.12xlarge\n",
    "             container_startup_health_check_timeout=1800\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineIterator:\n",
    "\n",
    "    def __init__(self, stream):\n",
    "        # Iterator to get bytes from stream \n",
    "        self.byte_iterator = iter(stream)  \n",
    "        # Buffer stream bytes until we get a full line\n",
    "        self.buffer = io.BytesIO()  \n",
    "      # Track current reading position within buffer\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Make class iterable \n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "           # Seek read position within buffer\n",
    "           self.buffer.seek(self.read_pos)  \n",
    "           # Try reading a line from current position\n",
    "           line = self.buffer.readline()\n",
    "           # If we have a full line\n",
    "           if line and line[-1] == ord('\\n'):\n",
    "               # Increment reading position past this line\n",
    "               self.read_pos += len(line)  \n",
    "               # Return the line read without newline char\n",
    "               return line[:-1] \n",
    "           # Fetch next chunk from stream  \n",
    "           try:\n",
    "               chunk = next(self.byte_iterator)\n",
    "           # Handle end of stream \n",
    "           except StopIteration:\n",
    "               # Check if we have any bytes still unread\n",
    "               if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                   continue\n",
    "               # If not, raise StopIteration\n",
    "               raise\n",
    "           # Add fetched bytes to end of buffer\n",
    "           self.buffer.seek(0, io.SEEK_END)  \n",
    "           self.buffer.write(chunk['PayloadPart']['Bytes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"Your task is to write a short paragraph in about 100 words about exercising regularly for a lifestyle focused website. Discuss benefits of regular exercises along with some tips for increasing exercise effectiveness\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<botocore.eventstream.EventStream at 0x7fcbb034dc90>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# set details: True as a runtime parameter within the input.\n",
    "body = {\"inputs\": prompt, \"parameters\": {\"max_new_tokens\":512, \"details\": True}}\n",
    "resp = sm_client.invoke_endpoint_with_response_stream(EndpointName=endpoint_name, Body=json.dumps(body), ContentType=\"application/json\")\n",
    "event_stream = resp['Body']\n",
    "event_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_log_prob = []\n",
    "\n",
    "for line in LineIterator(event_stream):\n",
    "    resp = json.loads(line)\n",
    "    \n",
    "    if resp['token'].get('text') != None:\n",
    "        token_log_prob = resp['token']['log_prob']\n",
    "        overall_log_prob.append(token_log_prob)\n",
    "    elif resp['generated_text'] != None:\n",
    "        generated_text= resp['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_text)\n",
    "overall_score=np.exp(np.mean(overall_log_prob))      \n",
    "print(f\"\\n\\nOverall confidence score in the generated text: {overall_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"Your task is to write a paragraph in about 500 words about exercising regularly for a lifestyle focused website. Discuss benefits of regular exercises along with some tips for increasing exercise effectiveness while reducing required time commitment\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: Your task is to write a short paragraph in about 100 words about exercising regularly for a lifestyle focused website. Discuss benefits of regular exercises along with some tips for increasing exercise effectiveness"
     ]
    }
   ],
   "source": [
    "def inference(payload):\n",
    "    # Call SageMaker endpoint and get response stream\n",
    "    resp = sm_client.invoke_endpoint_with_response_stream(EndpointName=endpoint_name, Body=json.dumps(payload), ContentType=\"application/json\")\n",
    "    event_stream = resp['Body']\n",
    "    text_output = []\n",
    "    for line in LineIterator(event_stream):\n",
    "        resp = json.loads(line) \n",
    "        # Extract text tokens if present\n",
    "        if resp['token'].get('text') != None:\n",
    "            token = resp['token']['text']\n",
    "            text_output.append(token)  \n",
    "            print(token, end='')\n",
    "        # Get finish reason if details present\n",
    "        if resp.get('details') != None:\n",
    "            finish_reason = resp['details']['finish_reason']\n",
    "            # Return extracted output, finish reason and token length\n",
    "            return payload['inputs'] + ''.join(text_output), finish_reason, len(text_output)\n",
    "\n",
    "# set details: True as a runtime parameter within the input.\n",
    "payload = {\"inputs\": prompt,  \"parameters\": {\"max_new_tokens\":256, \"details\": True}} \n",
    "\n",
    "finish_reason = \"length\"\n",
    "# Print initial output \n",
    "print(f\"Output: {payload['inputs']}\", end='')  \n",
    "total_tokens = 0\n",
    "total_requests = 0\n",
    "while finish_reason == 'length':\n",
    "    # Call inference and get extracts\n",
    "    output_text, finish_reason, out_token_len = inference(payload)\n",
    "    # Update payload for next request\n",
    "    payload['inputs'] = output_text \n",
    "    total_tokens += out_token_len\n",
    "    total_requests += 1\n",
    "# Print metrics\n",
    "print(f\"\\n\\ntotal tokens generated: {total_tokens} \\ntotal requests sent: {total_requests}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
