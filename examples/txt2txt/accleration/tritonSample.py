import torch
import triton
import triton.language as tl
import numpy as np
import pycuda.driver as cuda
import pycuda.autoinit
from pycuda.compiler import SourceModule

# 1. PyTorch Implementation using custom CUDA backend
class MatmulCUDAFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, a, b):
        return torch.mm(a, b)  # Using lower level mm instead of matmul

# 2. Optimized CUDA Implementation with shared memory and tiling
cuda_kernel = """
#define BLOCK_SIZE 32

__global__ void matmul_kernel_optimized(float* __restrict__ a, 
                                      float* __restrict__ b, 
                                      float* __restrict__ c, 
                                      int M, int N, int K) {
    __shared__ float shared_a[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ float shared_b[BLOCK_SIZE][BLOCK_SIZE];
    
    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;
    
    int row = by * BLOCK_SIZE + ty;
    int col = bx * BLOCK_SIZE + tx;
    
    float acc = 0.0f;
    
    for (int tile = 0; tile < (K + BLOCK_SIZE - 1) / BLOCK_SIZE; ++tile) {
        // Collaborative loading of A and B tiles into shared memory
        if (row < M && tile * BLOCK_SIZE + tx < K)
            shared_a[ty][tx] = a[row * K + tile * BLOCK_SIZE + tx];
        else
            shared_a[ty][tx] = 0.0f;
            
        if (tile * BLOCK_SIZE + ty < K && col < N)
            shared_b[ty][tx] = b[(tile * BLOCK_SIZE + ty) * N + col];
        else
            shared_b[ty][tx] = 0.0f;
            
        __syncthreads();
        
        #pragma unroll
        for (int k = 0; k < BLOCK_SIZE; ++k) {
            acc += shared_a[ty][k] * shared_b[k][tx];
        }
        __syncthreads();
    }
    
    if (row < M && col < N) {
        c[row * N + col] = acc;
    }
}
"""

# 3. Optimized Triton Implementation with similar tiling strategy
@triton.jit
def matmul_kernel_optimized(
    a_ptr, b_ptr, c_ptr,
    M, N, K,
    BLOCK_SIZE_M: tl.constexpr, 
    BLOCK_SIZE_N: tl.constexpr, 
    BLOCK_SIZE_K: tl.constexpr,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
):
    # Similar tiling strategy as CUDA kernel
    pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    
    # 2D grid ordering
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n

    # Block offset with boundary checking
    offs_am = tl.arange(0, BLOCK_SIZE_M)
    offs_bn = tl.arange(0, BLOCK_SIZE_N)
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    
    # Add offsets
    offs_am = pid_m * BLOCK_SIZE_M + offs_am
    offs_bn = pid_n * BLOCK_SIZE_N + offs_bn

    # Initialize accumulator with higher precision
    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
    
    # Pointers to shared memory blocks
    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        k_idx = k * BLOCK_SIZE_K + offs_k
        # Boundary masks for A and B
        mask_a = (offs_am[:, None] < M) & (k_idx[None, :] < K)
        mask_b = (k_idx[:, None] < K) & (offs_bn[None, :] < N)
        
        # Load blocks with boundary checking and zero padding
        a = tl.load(a_ptr + offs_am[:, None] * stride_am + k_idx[None, :] * stride_ak, 
                   mask=mask_a, other=0.0)
        b = tl.load(b_ptr + k_idx[:, None] * stride_bk + offs_bn[None, :] * stride_bn, 
                   mask=mask_b, other=0.0)
        
        # Accumulate with higher precision
        accumulator += tl.dot(a, b)

    # Store results with boundary checking
    mask_c = (offs_am[:, None] < M) & (offs_bn[None, :] < N)
    c_ptrs = c_ptr + offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn
    tl.store(c_ptrs, accumulator, mask=mask_c)

def benchmark_comparison():
    print("Starting benchmark comparison...")
    
    # Common parameters for all implementations
    BLOCK_SIZE = 32
    NUM_WARMUP = 20  # Increased warmup iterations
    NUM_ITERATIONS = 100
    DTYPE = torch.float32
    RTOL = 1e-2  # Relaxed tolerance
    ATOL = 1e-2

    # Test different matrix sizes
    sizes = [
        (1024, 1024, 1024),
        (2048, 2048, 2048),
        (4096, 4096, 4096)
    ]

    for M, N, K in sizes:
        print(f"\nBenchmarking size: {M}x{N}x{K}")
        
        # Initialize matrices with controlled values
        torch.manual_seed(0)
        # Use smaller values to reduce numerical errors
        a = torch.randn(M, K, device='cuda', dtype=DTYPE) * 0.01
        b = torch.randn(K, N, device='cuda', dtype=DTYPE) * 0.01

        # Compute reference result with PyTorch
        reference = torch.mm(a, b)

        print("Performing warmup runs...")
        # Warmup all implementations
        for i in range(NUM_WARMUP):
            warmup_result = torch.mm(a, b)
            # Verify first warmup iteration
            if i == 0:  # Changed from comparing tensor to comparing iteration count
                assert torch.allclose(warmup_result, reference, rtol=RTOL, atol=ATOL), "PyTorch warmup inconsistent"
        torch.cuda.synchronize()

        # 1. PyTorch Implementation
        pytorch_times = []
        c_pytorch = None
        print("\nRunning PyTorch implementation...")
        for i in range(NUM_ITERATIONS):
            start = torch.cuda.Event(enable_timing=True)
            end = torch.cuda.Event(enable_timing=True)
            start.record()
            c_pytorch = MatmulCUDAFunction.apply(a, b)
            end.record()
            torch.cuda.synchronize()
            pytorch_times.append(start.elapsed_time(end))
            
            # Verify consistency every 10 iterations
            if i % 10 == 0:
                assert torch.allclose(c_pytorch, reference, rtol=RTOL, atol=ATOL), \
                    f"PyTorch result mismatch at iteration {i}"

        # 2. CUDA Implementation
        print("\nRunning CUDA implementation...")
        mod = SourceModule(cuda_kernel)
        matmul_cuda = mod.get_function("matmul_kernel_optimized")
        
        # Allocate memory
        a_cpu = a.detach().cpu().numpy()
        b_cpu = b.detach().cpu().numpy()
        c_cpu = np.empty((M, N), dtype=np.float32)
        
        a_gpu = cuda.mem_alloc(a_cpu.nbytes)
        b_gpu = cuda.mem_alloc(b_cpu.nbytes)
        c_gpu = cuda.mem_alloc(c_cpu.nbytes)
        
        cuda.memcpy_htod(a_gpu, a_cpu)
        cuda.memcpy_htod(b_gpu, b_cpu)
        
        cuda_times = []
        c_cuda = None
        for i in range(NUM_ITERATIONS):
            start = cuda.Event()
            end = cuda.Event()
            start.record()
            
            matmul_cuda(
                a_gpu, b_gpu, c_gpu,
                np.int32(M), np.int32(N), np.int32(K),
                block=(BLOCK_SIZE, BLOCK_SIZE, 1),
                grid=((N + BLOCK_SIZE - 1) // BLOCK_SIZE,
                     (M + BLOCK_SIZE - 1) // BLOCK_SIZE)
            )
            
            end.record()
            end.synchronize()
            cuda_times.append(start.time_till(end))
            
            # Verify every 10 iterations
            if i % 10 == 0:
                cuda.memcpy_dtoh(c_cpu, c_gpu)
                c_cuda = torch.from_numpy(c_cpu).cuda()
                try:
                    assert torch.allclose(c_cuda, reference, rtol=RTOL, atol=ATOL), \
                        f"CUDA result mismatch at iteration {i}"
                except AssertionError as e:
                    print(f"CUDA verification failed at iteration {i}")
                    max_diff = torch.max(torch.abs(reference - c_cuda))
                    print(f"Maximum absolute difference: {max_diff.item()}")
                    cuda_times = []  # Invalidate results
                    break

        # 3. Triton Implementation
        print("\nRunning Triton implementation...")
        triton_times = []
        c_triton = torch.empty((M, N), device='cuda', dtype=DTYPE)
        grid = (triton.cdiv(M, BLOCK_SIZE) * triton.cdiv(N, BLOCK_SIZE),)
        
        for i in range(NUM_ITERATIONS):
            start = torch.cuda.Event(enable_timing=True)
            end = torch.cuda.Event(enable_timing=True)
            start.record()
            matmul_kernel_optimized[grid](
                a_ptr=a, b_ptr=b, c_ptr=c_triton,
                M=M, N=N, K=K,
                BLOCK_SIZE_M=BLOCK_SIZE,
                BLOCK_SIZE_N=BLOCK_SIZE,
                BLOCK_SIZE_K=BLOCK_SIZE,
                stride_am=K, stride_ak=1,
                stride_bk=N, stride_bn=1,
                stride_cm=N, stride_cn=1,
            )
            end.record()
            torch.cuda.synchronize()
            triton_times.append(start.elapsed_time(end))
            
            # Verify every 10 iterations
            if i % 10 == 0:
                try:
                    assert torch.allclose(c_triton, reference, rtol=RTOL, atol=ATOL), \
                        f"Triton result mismatch at iteration {i}"
                except AssertionError as e:
                    print(f"Triton verification failed at iteration {i}")
                    max_diff = torch.max(torch.abs(reference - c_triton))
                    print(f"Maximum absolute difference: {max_diff.item()}")
                    triton_times = []  # Invalidate results
                    break

        # Print results only if all implementations passed verification
        if pytorch_times and cuda_times and triton_times:
            def print_stats(name, times):
                mean = np.mean(times)
                std = np.std(times)
                tflops = 2 * M * N * K * 1e-12 / (mean * 1e-3)
                print(f"{name:>10} - Mean: {mean:>8.2f} ms (Â±{std:>6.2f}), {tflops:>6.2f} TFLOPS")

            print("\nResults:")
            print_stats("PyTorch", pytorch_times)
            print_stats("CUDA", cuda_times)
            print_stats("Triton", triton_times)
        else:
            print("\nBenchmark skipped due to verification failures")

if __name__ == "__main__":
    benchmark_comparison()
