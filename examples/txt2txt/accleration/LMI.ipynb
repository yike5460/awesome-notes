{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker boto3 huggingface_hub awscli --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py310/lib/python3.10/site-packages/sagemaker/base_serializers.py:28: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 1.21.5)\n",
      "  import scipy.sparse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ubuntu/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import jinja2\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.setup_default_session(region_name='us-east-1')\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bucket = sess.default_bucket()  # bucket to house model artifacts\n",
    "s3_code_prefix = \"hf-large-model-djl/meta-llama/Llama-2-7b-fp16/code\"  # folder within bucket where code artifact will go\n",
    "\n",
    "s3_model_prefix = \"hf-large-model-djl/meta-llama/Llama-2-7b-fp16/model\"  # folder within bucket where model artifact will go\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "jinja_env = jinja2.Environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepspeed_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\", \n",
    "    region=sess.boto_session.region_name, \n",
    "    version=\"0.26.0\"\n",
    ")\n",
    "\n",
    "env_generation = {\"HUGGINGFACE_HUB_CACHE\": \"/tmp\",\n",
    "                  \"TRANSFORMERS_CACHE\": \"/tmp\",\n",
    "                  \"SERVING_LOAD_MODELS\": \"test::Python=/opt/ml/model\",\n",
    "                  \"OPTION_MODEL_ID\": \"TheBloke/Llama-2-7B-Chat-fp16\",\n",
    "                  \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "                  \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "                  \"OPTION_ROLLING_BATCH\": \"vllm\",\n",
    "                  \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"32\",\n",
    "                  \"OPTION_DTYPE\":\"fp16\"\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trtllm_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-tensorrtllm\",\n",
    "    region=sess.boto_session.region_name,\n",
    "    version=\"0.26.0\"\n",
    ")\n",
    "\n",
    "\n",
    "env_summarization = {\"HUGGINGFACE_HUB_CACHE\": \"/tmp\",\n",
    "                     \"TRANSFORMERS_CACHE\": \"/tmp\",\n",
    "                     \"SERVING_LOAD_MODELS\": \"test::MPI=/opt/ml/model\",\n",
    "                     \"OPTION_MODEL_ID\": \"TheBloke/Llama-2-7b-fp16\",\n",
    "                     \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "                     \"OPTION_ROLLING_BATCH\": \"trtllm\",\n",
    "                     \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"64\"\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables are ---- > {'HUGGINGFACE_HUB_CACHE': '/tmp', 'TRANSFORMERS_CACHE': '/tmp', 'SERVING_LOAD_MODELS': 'test::Python=/opt/ml/model', 'OPTION_MODEL_ID': 'TheBloke/Llama-2-7B-Chat-fp16', 'OPTION_TRUST_REMOTE_CODE': 'true', 'OPTION_TENSOR_PARALLEL_DEGREE': 'max', 'OPTION_ROLLING_BATCH': 'vllm', 'OPTION_MAX_ROLLING_BATCH_SIZE': '32', 'OPTION_DTYPE': 'fp16'}\n",
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.26.0-deepspeed0.12.6-cu121\n"
     ]
    }
   ],
   "source": [
    "# - Select the appropriate environment variable which will tune the deployment server.\n",
    "env = env_generation # use this in case it is 'generation' task \n",
    "# env = env_summarization # enable this in case your use case is summarization ( high input and medium output sizes )\n",
    "\n",
    "# - now we select the appropriate container \n",
    "inference_image_uri = deepspeed_image_uri # use this in case it is 'generation' task \n",
    "#inference_image_uri = trtllm_image_uri # enable this in case your use case is summarization ( high input and medium output sizes ) \n",
    "\n",
    "\n",
    "print(f\"Environment variables are ---- > {env}\")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lmi-llama2-7b-2024-06-21-10-35-51-213\n",
      "Created Model: arn:aws:sagemaker:us-east-1:705247044519:model/lmi-llama2-7b-2024-06-21-10-35-51-213\n"
     ]
    }
   ],
   "source": [
    "model_name = sagemaker.utils.name_from_base(\"lmi-llama2-7b\")\n",
    "print(model_name)\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"Environment\": env,\n",
    "    }\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:705247044519:endpoint-config/lmi-llama2-7b-2024-06-21-10-35-51-213-config',\n",
       " 'ResponseMetadata': {'RequestId': '223e24b2-52e7-4fba-bff8-c93b838c3082',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '223e24b2-52e7-4fba-bff8-c93b838c3082',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '125',\n",
       "   'date': 'Fri, 21 Jun 2024 10:35:55 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g5.2xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            # \"ModelDataDownloadTimeoutInSeconds\": 2400,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 2400,\n",
    "            \"RoutingConfig\": {\n",
    "                'RoutingStrategy': 'LEAST_OUTSTANDING_REQUESTS'\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Endpoint: arn:aws:sagemaker:us-east-1:705247044519:endpoint/lmi-llama2-7b-2024-06-21-10-35-51-213-endpoint\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-east-1:705247044519:endpoint/lmi-llama2-7b-2024-06-21-10-35-51-213-endpoint\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this for Chatbot or QA or open ended generation task\n",
    "prompt = \"Amazon.com is the best\"\n",
    "params = { \"max_new_tokens\": 100,\"do_sample\": False }\n",
    "\n",
    "# # - use these for Summarization use case test \n",
    "# prompt = \"\"\"Briefly summarize this paragraph: Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document. Use Amazon Comprehend to create new products based on understanding the structure of documents. For example, using Amazon Comprehend you can search social networking feeds for mentions of products or scan an entire document repository for key phrases.\n",
    "# You can access Amazon Comprehend document analysis capabilities using the Amazon Comprehend console or using the Amazon Comprehend APIs. You can run real-time analysis for small workloads or you can start asynchronous analysis jobs for large document sets. You can use the pre-trained models that Amazon Comprehend provides, or you can train your own custom models for classification and entity recognition.\n",
    "# All of the Amazon Comprehend features accept UTF-8 text documents as the input. In addition, custom classification and custom entity recognition accept image files, PDF files, and Word files as input.\n",
    "# Amazon Comprehend can examine and analyze documents in a variety of languages, depending on the specific feature. For more information, see Languages supported in Amazon Comprehend. Amazon Comprehendâ€™s Dominant language capability can examine documents and determine the dominant language for a far wider selection of languages.\"\"\"\n",
    "# params = { \"max_new_tokens\":64, \"temperature\":0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.74 ms, sys: 4.78 ms, total: 14.5 ms\n",
      "Wall time: 3.13 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"generated_text\": \" place to buy a new laptop. According to CNET, Amazon offers a wide selection of laptops from top brands like Dell, HP, Lenovo, and more. Amazon also offers free shipping, streaming, and customer reviews to help you find the right laptop for your needs. Amazon.com has a wide selection of laptops from top brands like Dell, HP, Lenovo, and more. Additionally, Amazon offers free shipping and customer reviews to\"}'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": params\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '5e655787-12af-4c36-b156-f9444a9b5267',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '5e655787-12af-4c36-b156-f9444a9b5267',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Fri, 21 Jun 2024 11:34:36 GMT',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
