import re
from dotenv import load_dotenv
import boto3
import json
from openai import OpenAI
from termcolor import colored

load_dotenv()

# Initialize the LLM client
bedrock_runtime = boto3.client(service_name='bedrock-runtime', region_name="us-east-1")
client = OpenAI()

fix_prompt_template = """
Here are user original prompt: {_prompt} and response for an Claude3: {_response} and \n\nI have a few requirement of how Claude3 should responsded to original prompts and such prompt should be updated accordingly to align with OpenAI response. Here are the evaluations:\n{_evaluation_summary}\n\nPlease provide an improved version of the original prompt based on the evaluations. Please analyze the original prompt, response and user evaluation, then consider how the user original prompt can be improved accordingly, then respond with the revised prompt to help user improve in <fixed_prompt></fixed_prompt> tags.
Assistant: <fixed_prompt> revised prompt </fixed_prompt>
"""

bedrock_model_id = 'anthropic.claude-3-sonnet-20240229-v1:0'
openai_model_id = 'gpt-4-1106-preview'

default_system = "you have profound knowledge and hands on experience in field of software engineering and artificial intelligence, you are also an experienced solution architect in Amazon Web Service and have expertise to impelment model application development with AWS in consdieration of well architect and industry best practice."
bedrock_default_system = default_system
openai_default_system = default_system

def generate_openai_response(prompt):
    completion = client.chat.completions.create(
        model = openai_model_id,
        messages=[
            {"role": "system", "content": openai_default_system},
            {"role": "user", "content": prompt}
        ]
    )
    """
    ChatCompletionMessage(content='Hello! How can I assist you today with software engineering, artificial intelligence, or AWS solutions architecture? If you have any specific questions or need guidance, feel free to ask.', role='assistant', function_call=None, tool_calls=None)
    """
    return(completion.choices[0].message.content)
 
def generate_bedrock_response(prompt):
    """
    This function generates a test dataset by invoking a model with a given prompt.

    Parameters:
    prompt (str): The user input prompt.

    Returns:
    matches (list): A list of questions generated by the model, each wrapped in <case></case> XML tags.
    """
    message = {
        "role": "user",
        "content": [
            # {"type": "image", "source": {"type": "base64", "media_type": "image/jpeg", "data": content_image}},
            {"type": "text", "text": prompt}
        ]
    }
    messages = [message]
    body = json.dumps({
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": 4000,
        "messages": messages,
        "system": bedrock_default_system,
    })
    response = bedrock_runtime.invoke_model(body=body, modelId=bedrock_model_id)
    response_body = json.loads(response.get('body').read())
    return(response_body['content'][0]['text'])

def revise_bedrock_prompt(prompt, response):
    """
    Iterate through the evaluations and revise the prompt accordingly until the user input evaluations is null for satisfactory prompt.
    """
    evaluation_summary = []
    revised_prompt = prompt
    while True:
        evaluation = input(colored("Enter your evaluation (leave blank to finish): ", "yellow"))
        if evaluation == "":
            break
        # store the historical evaluations
        evaluation_summary.append(evaluation)
        print(colored(f"Evaluation Summary:\n{evaluation_summary}", "red"))
        message = {
            "role": "user",
            "content": [
                {"type": "text",
                # Note the prompt will be overwrite as the iteration start, each evaluation will consider the new revised prompt, but the evaluation will be stored continuouly.
                "text": fix_prompt_template.format(_prompt=prompt, _response=response, _evaluation_summary=evaluation_summary)}
            ]
        }
        messages = [message]
        body = json.dumps({
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 4000,
            "messages": messages,
            "system": default_system,
        })
        response = bedrock_runtime.invoke_model(body=body, modelId=bedrock_model_id)
        response_body = json.loads(response.get('body').read())
        pattern = r'<fixed_prompt>(.*?)</fixed_prompt>'
        matches = re.findall(pattern, response_body['content'][0]['text'], re.DOTALL)
        if matches:
            revised_prompt = matches[0]
            # invoke the bedrock api interatively
            response = generate_bedrock_response(revised_prompt)
            print("\nRevised response from Bedrock:")
            print(colored(response, "blue"))
            print("\nRevised prompt:")
            print(colored(revised_prompt, "yellow"))
        else:
            print(colored("No revision found.", "yellow"))
            break
    return revised_prompt

def main():
    initial_prompt = input("Enter the initial prompt for OpenAI: ")
    openai_result = generate_openai_response(initial_prompt)
    print("\nResponse from OpenAI:")
    print(colored(openai_result, "red"))
    bedrock_result = generate_bedrock_response(initial_prompt)
    print("\nResponse from Bedrock:")
    print(colored(bedrock_result, "blue"))
    revised_prompt = revise_bedrock_prompt(initial_prompt, bedrock_result)
    print("\nRevised prompt:")
    print(colored(revised_prompt, "yellow"))

if __name__ == "__main__":
    main()