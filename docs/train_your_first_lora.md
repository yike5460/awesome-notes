# Quick Start (simple & manual)
## Install Kohya

Install conda or venv to create a virtual environment for better package management. Refer to offcial [Kohya document](https://github.com/bmaltais/kohya_ss?tab=readme-ov-file#linux-pre-requirements) for more details.

```
git clone https://github.com/bmaltais/kohya_ss.git
cd kohya_ss
./gui.sh --server_port <your custom port> --inbrowser --share
# settup the env
accelerate config
------------------------------------------------------------------------------------In which compute environment are you running?
This machine                                                                        
------------------------------------------------------------------------------------Which type of machine are you using?
No distributed training                                                             
Do you want to run your training on CPU only (even if a GPU / Apple Silicon device is available)? [yes/NO]:
Do you wish to optimize your script with torch dynamo?[yes/NO]:                     
Do you want to use DeepSpeed? [yes/NO]:                                             
What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:
------------------------------------------------------------------------------------Do you wish to use FP16 or BF16 (mixed precision)?
bf16                                                                                
accelerate configuration saved at /home/ubuntu/.cache/huggingface/accelerate/default_config.yaml
```

## Prepare dataset and configuration
Open the browser with the link provided by the server, the link is similar to https://xx.gradio.live/

### [Optional] Use pre-defined configuration file [HERE](https://raw.githubusercontent.com/yike5460/just_notes/main/docs/loraTrainingConfig.json)

Define the path of the configuration file and press the load button
![Screenshot 2024-02-29 at 14 30 40](https://github.com/yike5460/just_notes/assets/23544182/fb865ebe-b5bf-480b-ad36-5cbdd50d54f6)

### Prepare the dataset you trained or use [SAMPLE IMAGES](https://github.com/yike5460/just_notes/tree/main/examples/LoRA) provided
The dataset mean the stylish imagess to you'd like to train your LoRA model on, the number of images can be around 10-20. Create 3 folders (image/log/model) and store your image in image sub-folder with naming as <training-step>_<model-name>, e.g 150_yike5460_model:

After image been prepared, you need to add caption per image to describe the content before actual trainning. You can first use BLIP provided in Kohya UI to automate partial of the process shown below:

Input the "Image folder to caption" and "Prefix to add to BLIP caption",  e.g tusuji, then execute the caption process and you will see new txt files been created with the same file name as image.

![image](https://github.com/yike5460/just_notes/assets/23544182/8fe50b78-eccf-41cb-b2af-ad54d374b1bf)

You need to ajust the content in each txt file since the content generated by model are not always correct enough. The final folder hierarchy is shown below:

```
.
├── image
│   ├── 150_yike5460_model
│   │   ├── 1.jpg
│   │   ├── 1.txt
│   │   ├── 10.jpg
│   │   ├── 10.txt
│   │   ├── 2.jpg
│   │   ├── 2.txt
│   │   ├── 3.png
│   │   ├── 3.txt
│   │   ├── 4.jpg
│   │   ├── 4.txt
│   │   ├── 5.png
│   │   ├── 5.txt
│   │   ├── 6.jpg
│   │   ├── 6.txt
│   │   ├── 7.jpg
│   │   ├── 7.txt
│   │   ├── 8.jpg
│   │   ├── 8.txt
│   │   ├── 9.jpg
│   │   └── 9.txt
│   └── 150_yike5460_tsj_model
│       ├── 00000-0-Screenshot 2023-10-20 at 11.16.09.png
│       ├── 00000-0-Screenshot 2023-10-20 at 11.16.09.txt
│       ├── 00001-0-Screenshot 2023-10-20 at 11.21.49.png
│       ├── 00001-0-Screenshot 2023-10-20 at 11.21.49.txt
│       ├── 00002-0-Screenshot 2023-10-20 at 11.20.28.png
│       ├── 00002-0-Screenshot 2023-10-20 at 11.20.28.txt
│       ├── 00003-0-Screenshot 2023-10-20 at 11.22.03.png
│       ├── 00003-0-Screenshot 2023-10-20 at 11.22.03.txt
│       ├── 00004-0-Screenshot 2023-10-20 at 11.21.42.png
│       ├── 00004-0-Screenshot 2023-10-20 at 11.21.42.txt
│       ├── 00005-0-Screenshot 2023-10-20 at 11.20.58.png
│       ├── 00005-0-Screenshot 2023-10-20 at 11.20.58.txt
│       ├── 00006-0-Screenshot 2023-10-20 at 11.21.29.png
│       ├── 00006-0-Screenshot 2023-10-20 at 11.21.29.txt
│       ├── 00007-0-Screenshot 2023-10-20 at 11.18.12.png
│       ├── 00007-0-Screenshot 2023-10-20 at 11.18.12.txt
│       ├── 00008-0-Screenshot 2023-10-20 at 11.22.16.png
│       ├── 00008-0-Screenshot 2023-10-20 at 11.22.16.txt
│       ├── 00009-0-Screenshot 2023-10-20 at 11.17.05.png
│       ├── 00009-0-Screenshot 2023-10-20 at 11.17.05.txt
│       ├── 00010-0-Screenshot 2023-10-20 at 11.16.39.png
│       ├── 00010-0-Screenshot 2023-10-20 at 11.16.39.txt
│       ├── 00011-0-tusiji_4.png
│       ├── 00011-0-tusiji_4.txt
│       ├── 00012-0-Screenshot 2023-10-20 at 11.19.21.png
│       ├── 00012-0-Screenshot 2023-10-20 at 11.19.21.txt
│       ├── 00013-0-Screenshot 2023-10-20 at 11.23.30.png
│       ├── 00013-0-Screenshot 2023-10-20 at 11.23.30.txt
│       ├── 00014-0-Screenshot 2023-10-20 at 11.22.26.png
│       ├── 00014-0-Screenshot 2023-10-20 at 11.22.26.txt
│       ├── 00015-0-Screenshot 2023-10-20 at 11.20.36.png
│       ├── 00015-0-Screenshot 2023-10-20 at 11.20.36.txt
│       ├── 00016-0-Screenshot 2023-10-20 at 11.17.25.png
│       ├── 00016-0-Screenshot 2023-10-20 at 11.17.25.txt
│       ├── 00017-0-Screenshot 2023-10-20 at 11.24.03.png
│       ├── 00017-0-Screenshot 2023-10-20 at 11.24.03.txt
│       ├── 00018-0-Screenshot 2023-10-20 at 11.23.53.png
│       ├── 00018-0-Screenshot 2023-10-20 at 11.23.53.txt
│       ├── 00019-0-Screenshot 2023-10-20 at 11.21.11.png
│       └── 00019-0-Screenshot 2023-10-20 at 11.21.11.txt
├── log
└── model
```

## Start training and verfiy in WebUI
Fill the Image/Output/Logging folder with the actual path and start training, the training time depend on the GPU type, training step, image number, could be minutes or hours, mine are around 30 minutes with A10.

<img width="1465" alt="image" src="https://github.com/yike5460/just_notes/assets/23544182/63d8670c-706e-470c-9044-d13db3814dc9">

You should see the model and its description file in the model folder like:
```
└── model
    ├── yike5460_LoRA_tsj_Model.safetensors
    ├── yike5460_LoRA_tsj_Model_20240229-032338.json
    └── sample
```

Install WebUI:
```
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git
cd stable-diffusion-webui/
pip install -r requirements.txt
./webui.sh --share
```

Open the browser with the link provided by the server, the link is similar to https://xx.gradio.live/. Copy the LoRA model into the model/LoRA path and load the model in the WebUI.

<p align="center">
<img width="647" alt="image" src="https://github.com/yike5460/just_notes/assets/23544182/0f8fdbf0-6e07-433d-bed1-aab0a6f6f683">
</p>

```
(py310) ubuntu@ip-172-31-29-233:~/stable-diffusion-webui/models/Lora$ ls -al
total 295164
drwxrwxr-x  2 ubuntu ubuntu      4096 Feb 29 05:03 .
drwxrwxr-x 14 ubuntu ubuntu      4096 Feb 29 01:33 ..
-rw-rw-r--  1 ubuntu ubuntu 151112840 Feb 29 05:03 kyiamzn_LoRA_tsj_Model.safetensors
-rw-rw-r--  1 ubuntu ubuntu      3402 Feb 29 05:03 kyiamzn_LoRA_tsj_Model_20240229-032338.json
```

Click the LoRA and you will see the prompt with text like <lora:kyiamzn_LoRA_tsj_Model:1> to invoke such LoRA model with detailed prompt as follows:

```
positive prompt:
An anime girl bats a ball of yarn in a cozy cat cafe with sparkling eyes.<lora:yike5460_LoRA_tsj_Model:0.8>

negative prompt:
bad anatomy, bad hands, three hands, three legs, bad arms, missing legs, missing arms, poorly drawn face, bad face, fused face, cloned face, worst face, three crus, extra crus, fused crus, worst feet, three feet, fused feet, fused thigh, three thigh, fused thigh, extra thigh, worst thigh, missing fingers, extra fingers, ugly fingers, long fingers, horn, realistic photo, extra eyes, huge eyes, 2girl, amputation, disconnected limbs
```

Check the tusiji styled images in the output to see if our LoRA model take effective

<img width="2508" alt="image" src="https://github.com/yike5460/just_notes/assets/23544182/769d2c07-1aec-4fa0-9cdf-5b3816e31829">

# Advanced exploration (more details & automation)
Refer to https://civitai.com/models/22530 to go though the detail of the LoRA model and its training process with boilerplate colab notebook. And there are [official Kohya document](https://github.com/bmaltais/kohya_ss/blob/master/docs/train_README-zh.md) to guide you on the whole training process.

And there are pre-build notebook in colab to automate the whole LoRA training process with seamless integration with Google drive, including dataset preparation, model training, model evaluation etc. You can refer to [Trainer](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb) and [Dataset Maker](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipyn).