# Quick Start (simple & manual)
## Install Kohya

Install conda or venv to create a virtual environment for better package management. Refer to offcial [Kohya document](https://github.com/bmaltais/kohya_ss?tab=readme-ov-file#linux-pre-requirements) for more details.

```
git clone https://github.com/bmaltais/kohya_ss.git
cd kohya_ss
./gui.sh --server_port <your custom port> --inbrowser --share
# settup the env
accelerate config
------------------------------------------------------------------------------------In which compute environment are you running?
This machine                                                                        
------------------------------------------------------------------------------------Which type of machine are you using?
No distributed training                                                             
Do you want to run your training on CPU only (even if a GPU / Apple Silicon device is available)? [yes/NO]:
Do you wish to optimize your script with torch dynamo?[yes/NO]:                     
Do you want to use DeepSpeed? [yes/NO]:                                             
What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:
------------------------------------------------------------------------------------Do you wish to use FP16 or BF16 (mixed precision)?
bf16                                                                                
accelerate configuration saved at /home/ubuntu/.cache/huggingface/accelerate/default_config.yaml
```

## Prepare dataset and configuration
Open the browser with the link provided by the server, the link is similar to https://xx.gradio.live/

### [Optional] Use pre-defined configuration file [HERE](https://raw.githubusercontent.com/yike5460/just_notes/main/docs/loraTrainingConfig.json)

Define the path of the configuration file and press the load button
![Screenshot 2024-02-29 at 14 30 40](https://github.com/yike5460/just_notes/assets/23544182/fb865ebe-b5bf-480b-ad36-5cbdd50d54f6)

### Prepare the dataset you trained or use [SAMPLE IMAGES](https://github.com/yike5460/just_notes/tree/main/examples/LoRA/sample_images) provided
The dataset mean the stylish imagess to you'd like to train your LoRA model on, the number of images can be around 10-20. Create 3 folders (image/log/model) and store your image in image sub-folder with naming as <training-step>_<model-name>, e.g 150_yike5460_model:

After image been prepared, you need to add caption per image to describe the content before actual trainning. You can first use BLIP provided in Kohya UI to automate partial of the process shown below:

Input the "Image folder to caption" and "Prefix to add to BLIP caption",  e.g tusuji, then execute the caption process and you will see new txt files been created with the same file name as image.

![image](https://github.com/yike5460/just_notes/assets/23544182/8fe50b78-eccf-41cb-b2af-ad54d374b1bf)

You need to ajust the content in each txt file since the content generated by model are not always correct enough. The final folder hierarchy is shown below:

```
.
├── image
│   ├── 150_yike5460_model
│   │   ├── 1.jpg
│   │   ├── 1.txt
│   │   ├── 10.jpg
│   │   ├── 10.txt
│   │   ├── 2.jpg
│   │   ├── 2.txt
│   │   ├── 3.png
│   │   ├── 3.txt
│   │   ├── 4.jpg
│   │   ├── 4.txt
│   │   ├── 5.png
│   │   ├── 5.txt
│   │   ├── 6.jpg
│   │   ├── 6.txt
│   │   ├── 7.jpg
│   │   ├── 7.txt
│   │   ├── 8.jpg
│   │   ├── 8.txt
│   │   ├── 9.jpg
│   │   └── 9.txt
│   └── 150_yike5460_tsj_model
│       ├── 00000-0-Screenshot 2023-10-20 at 11.16.09.png
│       ├── 00000-0-Screenshot 2023-10-20 at 11.16.09.txt
│       ├── 00001-0-Screenshot 2023-10-20 at 11.21.49.png
│       ├── 00001-0-Screenshot 2023-10-20 at 11.21.49.txt
│       ├── 00002-0-Screenshot 2023-10-20 at 11.20.28.png
│       ├── 00002-0-Screenshot 2023-10-20 at 11.20.28.txt
│       ├── 00003-0-Screenshot 2023-10-20 at 11.22.03.png
│       ├── 00003-0-Screenshot 2023-10-20 at 11.22.03.txt
│       ├── 00004-0-Screenshot 2023-10-20 at 11.21.42.png
│       ├── 00004-0-Screenshot 2023-10-20 at 11.21.42.txt
│       ├── 00005-0-Screenshot 2023-10-20 at 11.20.58.png
│       ├── 00005-0-Screenshot 2023-10-20 at 11.20.58.txt
│       ├── 00006-0-Screenshot 2023-10-20 at 11.21.29.png
│       ├── 00006-0-Screenshot 2023-10-20 at 11.21.29.txt
│       ├── 00007-0-Screenshot 2023-10-20 at 11.18.12.png
│       ├── 00007-0-Screenshot 2023-10-20 at 11.18.12.txt
│       ├── 00008-0-Screenshot 2023-10-20 at 11.22.16.png
│       ├── 00008-0-Screenshot 2023-10-20 at 11.22.16.txt
│       ├── 00009-0-Screenshot 2023-10-20 at 11.17.05.png
│       ├── 00009-0-Screenshot 2023-10-20 at 11.17.05.txt
│       ├── 00010-0-Screenshot 2023-10-20 at 11.16.39.png
│       ├── 00010-0-Screenshot 2023-10-20 at 11.16.39.txt
│       ├── 00011-0-tusiji_4.png
│       ├── 00011-0-tusiji_4.txt
│       ├── 00012-0-Screenshot 2023-10-20 at 11.19.21.png
│       ├── 00012-0-Screenshot 2023-10-20 at 11.19.21.txt
│       ├── 00013-0-Screenshot 2023-10-20 at 11.23.30.png
│       ├── 00013-0-Screenshot 2023-10-20 at 11.23.30.txt
│       ├── 00014-0-Screenshot 2023-10-20 at 11.22.26.png
│       ├── 00014-0-Screenshot 2023-10-20 at 11.22.26.txt
│       ├── 00015-0-Screenshot 2023-10-20 at 11.20.36.png
│       ├── 00015-0-Screenshot 2023-10-20 at 11.20.36.txt
│       ├── 00016-0-Screenshot 2023-10-20 at 11.17.25.png
│       ├── 00016-0-Screenshot 2023-10-20 at 11.17.25.txt
│       ├── 00017-0-Screenshot 2023-10-20 at 11.24.03.png
│       ├── 00017-0-Screenshot 2023-10-20 at 11.24.03.txt
│       ├── 00018-0-Screenshot 2023-10-20 at 11.23.53.png
│       ├── 00018-0-Screenshot 2023-10-20 at 11.23.53.txt
│       ├── 00019-0-Screenshot 2023-10-20 at 11.21.11.png
│       └── 00019-0-Screenshot 2023-10-20 at 11.21.11.txt
├── log
└── model
```

## Start training and verfiy in WebUI
Fill the Image/Output/Logging folder with the actual path and start training, the training time depend on the GPU type, training step, image number, could be minutes or hours, mine are around 30 minutes with A10.

<img width="1465" alt="image" src="https://github.com/yike5460/just_notes/assets/23544182/63d8670c-706e-470c-9044-d13db3814dc9">

You should see the model and its description file in the model folder like:
```
└── model
    ├── yike5460_LoRA_tsj_Model.safetensors
    ├── yike5460_LoRA_tsj_Model_20240229-032338.json
    └── sample
```

Install WebUI:
```
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git
cd stable-diffusion-webui/
pip install -r requirements.txt
./webui.sh --share
```

Open the browser with the link provided by the server, the link is similar to https://xx.gradio.live/. Copy the LoRA model into the model/LoRA path and load the model in the WebUI.

<p align="center">
<img width="647" alt="image" src="https://github.com/yike5460/just_notes/assets/23544182/0f8fdbf0-6e07-433d-bed1-aab0a6f6f683">
</p>

```
(py310) ubuntu@ip-172-31-29-233:~/stable-diffusion-webui/models/Lora$ ls -al
total 295164
drwxrwxr-x  2 ubuntu ubuntu      4096 Feb 29 05:03 .
drwxrwxr-x 14 ubuntu ubuntu      4096 Feb 29 01:33 ..
-rw-rw-r--  1 ubuntu ubuntu 151112840 Feb 29 05:03 kyiamzn_LoRA_tsj_Model.safetensors
-rw-rw-r--  1 ubuntu ubuntu      3402 Feb 29 05:03 kyiamzn_LoRA_tsj_Model_20240229-032338.json
```

Click the LoRA and you will see the prompt with text like <lora:kyiamzn_LoRA_tsj_Model:1> to invoke such LoRA model with detailed prompt as follows:

```
positive prompt:
An anime girl bats a ball of yarn in a cozy cat cafe with sparkling eyes.<lora:yike5460_LoRA_tsj_Model:0.8>

negative prompt:
bad anatomy, bad hands, three hands, three legs, bad arms, missing legs, missing arms, poorly drawn face, bad face, fused face, cloned face, worst face, three crus, extra crus, fused crus, worst feet, three feet, fused feet, fused thigh, three thigh, fused thigh, extra thigh, worst thigh, missing fingers, extra fingers, ugly fingers, long fingers, horn, realistic photo, extra eyes, huge eyes, 2girl, amputation, disconnected limbs
```

Check the tusiji styled images in the output to see if our LoRA model take effective

<img width="2508" alt="image" src="https://github.com/yike5460/just_notes/assets/23544182/769d2c07-1aec-4fa0-9cdf-5b3816e31829">

# Use Claude3 and Stable Diffusion Extension AWS (performant & automation)
## Install Stable Diffusion Extension on AWS
```
cd stable-diffusion-aws-extension
git checkout kohya
cd infrastructure
npx npm install
cdk deploy --parameters Email=<email_address> --parameters Bucket=<your_bucket_name> --parameters EcrImageTag=v1.4.0-7503bc6 --rollback true --require-approval never
```
Record the API URL(ApiGatewayUrl) and the S3 bucket <your_bucket_name> for the next step.
```
Outputs:
Extension-for-Stable-Diffusion-on-AWS.ApiGatewayUrl = https://xxxx.execute-api.us-east-1.amazonaws.com/prod/
Extension-for-Stable-Diffusion-on-AWS.ApiGatewayUrlToken = 09876543210987654321
Extension-for-Stable-Diffusion-on-AWS.S3BucketName = your_bucket_name
Extension-for-Stable-Diffusion-on-AWS.SNSTopicName = StableDiffusionSnsUserTopic
Extension-for-Stable-Diffusion-on-AWS.sdextensiondeployapiEndpoint7DE80613 = https://xxxx.execute-api.us-east-1.amazonaws.com/prod/
Stack ARN:
arn:aws:cloudformation:us-east-1:705247044519:stack/Extension-for-Stable-Diffusion-on-AWS/ec379cf0-e276-11ee-9737-12e433916ecd
```

## Prepare the dataset and model
Download SDXL or SD 1.5 models by below commands, you can also download the model from community.
```
wget -qP models/Stable-diffusion/ https://aws-gcr-solutions-us-east-1.s3.us-east-1.amazonaws.com/extension-for-stable-diffusion-on-aws/models/Stable-diffusion/sd_xl_base_1.0.safetensors
wget -qP models/Stable-diffusion/ https://aws-gcr-solutions-us-east-1.s3.us-east-1.amazonaws.com/extension-for-stable-diffusion-on-aws/models/Stable-diffusion/v1-5-pruned-emaonly.safetensors
```

Upload model to the S3 bucket <your_bucket_name>
```
aws s3 cp *.safetensors s3://<your_bucket_name>/sd/model
```

Prepare the dataset using script in [scrape image](../examples/LoRA/customSearch.py) and add the image caption using script in [BLIP](../examples/LoRA/imageCaption.py)
```
python customSearch.py

Enter the label (e.g., 'jeff bezos', 'taylor swift'): tarlor swift
Enter the image size (e.g., 'small', 'medium', 'large', 'xlarge'): medium
Enter the image type (e.g., 'face', 'photo', 'clipart', 'lineart'): face
Enter the image color type (e.g., 'color', 'gray', 'mono', 'trans'): 
Enter the image dominant color (e.g., 'black', 'blue', 'brown', 'gray'): 
Enter the license type (e.g., 'cc_publicdomain', 'cc_attribute', 'cc_sharealike', 'cc_noncommercial', 'cc_nonderived'):
```
You will get images download in folder downloaded_images/ and you can use the Claude3 to add caption to the images automatically.
```
python imageCaption.py
```

Then images and caption txt file describe the image will be prepared in the folder, you can upload the folder to the S3 bucket <your_bucket_name> and start the training process using API URL generated by the CDK deployment.

```
ls downloaded_images/
image_0.jpg  image_1.txt  image_3.jpg  image_4.txt  image_6.jpg  image_7.txt  image_9.jpg
image_0.txt  image_2.jpg  image_3.txt  image_5.jpg  image_6.txt  image_8.jpg  image_9.txt
image_1.jpg  image_2.txt  image_4.jpg  image_5.txt  image_7.jpg  image_8.txt

aws s3 sync downloaded_images s3://<your_bucket_name>/sd/dataset/100_lora-model/
```

Note some calculations here, the naming rule of the dataset folder is <repeat num>_<name>, assume there are 10 images in downloaded_images folder, and set it to 100_lora-model with 2000 training steps, it will calculate that there num_images * num_repeats = 100, which would be one epoch and 2000 training steps there mean 2000/100 = 20 epochs, and 20 states will be saved. Repeats are more important when we multiple folders inside of parent image folder, e.g. if one folders has 100 images and the other one only half of that, we can name the preceding folder with name 1_<name> and latter one with 2_<name> to do 2X repeat, so that both folders are trained for 100 steps per epoch.

## Start training and verfiy in WebUI
Call the API to start the training process, the API URL is similar to https://xxxx.execute-api.us-east-1.amazonaws.com/prod/trainings, and the payload is similar below:
```
{
    "params": {
        "training_params": {
            "training_instance_type": "ml.g5.2xlarge",
            "fm_type": "sd_xl",
            "s3_model_path": "s3://<your_bucket_name>/sd/model/sd_xl_base_1.0.safetensors",
            "s3_data_path": "s3://<your_bucket_name>/sd/dataset/100_lora-model"
        },
        "config_params": {
            "training": {
                "output_name": "100_lora-model-output",
                "max_train_epochs": 20,
                "optimizer_type": "AdamW8bit"
            }
        }
    }
}
```

The request should contain header "x-api-key" with the value of the token configured in the CDK deployment (09876543210987654321 by default), and the response will contain the training parameters and the training status.

```
{
    "statusCode": 200,
    "data": {
        "id": "9a157414-3a59-4c49-9592-35b9bdc611be",
        "status": "Training",
        "created": "1710495598.646034",
        "params": {
            "config_params": {
                "training": {
                    "output_name": "100_lora-model-output",
                    "max_train_epochs": "20",
                    "optimizer_type": "AdamW8bit"
                }
            },
            "training_params": {
                "s3_data_path": "s3://<your_bucket_name>/sd/dataset/100_lora-model",
                "training_instance_type": "ml.g5.2xlarge",
                "s3_model_path": "s3://<your_bucket_name>/sd/model/sd_xl_base_1.0.safetensors",
                "fm_type": "sd_xl",
                "s3_toml_path": "s3://<your_bucket_name>/kohya/train/9a157414-3a59-4c49-9592-35b9bdc611be/input/kohya_config_cloud_xl.toml"
            },
            "training_type": "kohya"
        },
        "input_location": "s3://<your_bucket_name>/kohya/train/9a157414-3a59-4c49-9592-35b9bdc611be/input",
        "output_location": "s3://<your_bucket_name>/kohya/train/9a157414-3a59-4c49-9592-35b9bdc611be/output"
    },
    "message": "OK"
}
```

You can check the training status in AWS SageMaker training console and the S3 bucket <your_bucket_name>, and the model will be saved in the output_location/model folder after the training process is done. Then you can use the WebUI to load the model and verify the model performance as the previous section.


# Advanced exploration (more details & automation)
Refer to https://civitai.com/models/22530 to go though the detail of the LoRA model and its training process with boilerplate colab notebook. And there are [official Kohya document](https://github.com/bmaltais/kohya_ss/blob/master/docs/train_README-zh.md) to guide you on the whole training process.

And there are pre-build notebook in colab to automate the whole LoRA training process with seamless integration with Google drive, including dataset preparation, model training, model evaluation etc. You can refer to [Trainer](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb) and [Dataset Maker](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipyn).